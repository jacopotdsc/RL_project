{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Initialization"
      ],
      "metadata": {
        "id": "zSHiaIsp_hxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ccdKZ4CltkW3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "DmPSS_NatdBk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn.preprocessing\n",
        "from sklearn.kernel_approximation import RBFSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Refo9Y_vtdBn"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Eq5dx6SGtdBp"
      },
      "outputs": [],
      "source": [
        "samples = [[-0.6810, -0.7323, -0.3519],\n",
        "        [ 0.9754, -0.2205, -0.9598],\n",
        "        [ 0.8309, -0.5564, -0.8232],\n",
        "        [ 0.4757,  0.8796,  0.8216],\n",
        "        [-0.9221, -0.3869,  0.8180],\n",
        "        [-0.9108, -0.4129,  0.2524],\n",
        "        [-0.1120, -0.9937, -0.8391],\n",
        "        [ 0.5760,  0.8175,  0.6597],\n",
        "        [ 0.4632, -0.8863, -0.1590],\n",
        "        [ 0.5347,  0.8450, -0.4712],\n",
        "        [ 0.5242,  0.8516, -0.9269],\n",
        "        [-0.8494, -0.5277,  0.9623],\n",
        "        [ 0.8243, -0.5661, -0.8888],\n",
        "        [-0.9457, -0.3252, -0.5394],\n",
        "        [ 0.9998, -0.0188, -0.7901],\n",
        "        [-0.9918, -0.1282,  0.9201]]\n",
        "\n",
        "num_envs = 16\n",
        "\n",
        "env1_name = \"Pendulum-v1\"\n",
        "env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "env1 = gym.make(env1_name)\n",
        "env2 = gym.make(env2_name)\n",
        "\n",
        "envs1 = make_vec_env(env1_name, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_name, n_envs=num_envs)\n",
        "\n",
        "p_envs_ids = [env1_name, env2_name ]\n",
        "p_envs = {env1_name: envs1,\n",
        "          env2_name: envs2\n",
        "          }\n",
        "\n",
        "#model_ac = ActorCritic(envs=p_envs, env_ids=p_envs_ids, hidden_size=256).to(device)\n",
        "#model_input = model_ac.create_model_input(samples, env1_name)\n",
        "\n",
        "#dist, value = model_ac(model_input)\n",
        "#print(f\"dist: {dist}, val: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Auxiliar function"
      ],
      "metadata": {
        "id": "Fy_Ly0wC_Yiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "zbZ12TwvtdBr"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "def plot(name, frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title(name + ': ' + 'frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "\n",
        "def test_env(model, env, id_env, max_step, vis=False):\n",
        "\n",
        "    state = torch.FloatTensor(env.reset()[0]).unsqueeze(0)\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    totalreward = 0\n",
        "    step = 0\n",
        "    while not done:\n",
        "        #state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        #print(state)\n",
        "        #state = torch.FloatTensor(state).to(device)\n",
        "        #dist,  = model(model_input)\n",
        "        state = np.array(state)\n",
        "        model_input = model.create_model_input(state, idenv)\n",
        "        dist,  = model(model_input)\n",
        "        nextstate, reward, terminated, truncated,  = env.step(dist.sample().cpu().numpy())\n",
        "        done = terminated or truncated\n",
        "\n",
        "\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        #state = np.expand_dims( np.array(next_state.squeeze(-1)), 0 )\n",
        "        #print(f\"sq: {[next_state.squeeze(-1)]}\")\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "        if step >= max_step:\n",
        "            break\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "def test_env(model, env, id_env, vis=False):\n",
        "\n",
        "    state = torch.FloatTensor(env.reset()[0]).unsqueeze(0)\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        #state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        #print(state)\n",
        "        #state = torch.FloatTensor(state).to(device)\n",
        "        #dist, _ = model(model_input)\n",
        "        state = np.array(state)\n",
        "        model_input = model.create_model_input(state, id_env)\n",
        "        dist, _ = model(model_input)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(dist.sample().cpu().numpy())\n",
        "        done = terminated or truncated\n",
        "\n",
        "\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        state = np.expand_dims( np.array(next_state.squeeze(-1)), 0 )\n",
        "        #print(f\"sq: {[next_state.squeeze(-1)]}\")\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "p_env1_name = \"Pendulum-v1\"\n",
        "p_env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "p_env1 = gym.make(p_env1_name)\n",
        "p_env2 = gym.make(p_env2_name)\n",
        "\n",
        "#test_env(model_PC, env1, env1_name )\n",
        "#test_env(model_PC, env2, env2_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Advantage Estimator**"
      ],
      "metadata": {
        "id": "xKeApqVRreDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "5pIDF5hftdBs"
      },
      "outputs": [],
      "source": [
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8L3VZyPtdBq"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "lcolBdfL-qd2"
      },
      "outputs": [],
      "source": [
        "class RBFFeatureEncoder:\n",
        "    def __init__(self, env1, env2, env3, n_component=100):\n",
        "\n",
        "        self.env1 = env1\n",
        "        self.env2 = env2\n",
        "        self.env3 = env3\n",
        "\n",
        "        data1 = np.array([env1.observation_space.sample() for x in range(10000)]); data1 = np.clip(data1, -1e38, 1e38)\n",
        "        data2 = np.array([env2.observation_space.sample() for x in range(10000)]); data2 = np.clip(data2, -1e38, 1e38)\n",
        "        data3 = np.array([env3.observation_space.sample() for x in range(10000)]); data3 = np.clip(data3, -1e38, 1e38)\n",
        "\n",
        "        self.rbf_sampler1 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler2 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler3 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "\n",
        "        self.standard_scaler1 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler2 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler3 = sklearn.preprocessing.StandardScaler()\n",
        "\n",
        "        self.standard_scaler1.fit(data1)\n",
        "        self.standard_scaler2.fit(data2)\n",
        "        self.standard_scaler3.fit(data3)\n",
        "\n",
        "        transformed_data1 = self.standard_scaler1.transform(data1)\n",
        "        transformed_data2 = self.standard_scaler2.transform(data2)\n",
        "        transformed_data3 = self.standard_scaler3.transform(data3)\n",
        "\n",
        "        self.rbf_sampler1.fit( transformed_data1 )\n",
        "        self.rbf_sampler2.fit( transformed_data2 )\n",
        "        self.rbf_sampler3.fit( transformed_data3 )\n",
        "\n",
        "    def wrapper_encode(self, model, model_input):\n",
        "\n",
        "        state = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "\n",
        "        if model.env1_id == env_id:\n",
        "            transformed_state = self.standard_scaler1.transform([state])\n",
        "            encoded_state = self.rbf_sampler1.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env2_id == env_id:\n",
        "            transformed_state = self.standard_scaler2.transform([state])\n",
        "            encoded_state = self.rbf_sampler2.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env3_id == env_id:\n",
        "            transformed_state = self.standard_scaler3.transform([state])\n",
        "            encoded_state = self.rbf_sampler3.transform(transformed_state).flatten()\n",
        "\n",
        "        return encoded_state\n",
        "\n",
        "    def encode(self, model, model_input):\n",
        "\n",
        "        states = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "        encoded_states = []\n",
        "\n",
        "        for s in states:\n",
        "            new_input = model.create_model_input(s,env_id )\n",
        "            encoded_states.append( self.wrapper_encode(model, new_input) )\n",
        "\n",
        "        return torch.tensor(encoded_states, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self.rbf_sampler1.n_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "1-VdeX7z-qd4"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "        state = self.output_layer(state)\n",
        "        return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ZZfIK7Nu-qd5"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_size, env_outputs, hidden_size, env_ids):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "        self.env1_actions = env_outputs[0]\n",
        "        self.env2_actions = env_outputs[1]\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        self.env1_output = nn.Linear(hidden_size, self.env1_actions)\n",
        "        self.env2_output = nn.Linear(hidden_size, self.env2_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "\n",
        "        if env_id == self.env1_id:   mu = self.env1_output(state)\n",
        "        elif env_id == self.env2_id: mu = self.env2_output(state)\n",
        "\n",
        "        return mu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "llFUdL-x-qd6"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, envs, env_ids, hidden_size, std=0.0, lr=0.0001):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "\n",
        "        self.envs1   = envs[self.env1_id]\n",
        "        self.envs2   = envs[self.env2_id]\n",
        "\n",
        "        self.env1_output = self.envs1.action_space.shape[0]\n",
        "        self.env2_output = self.envs2.action_space.shape[0]\n",
        "\n",
        "        self.encoder    = RBFFeatureEncoder(self.envs1, self.envs2, self.envs2)\n",
        "        self.input_size = self.encoder.size\n",
        "\n",
        "        #print(f\"num_inputs1: {self.input_size}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "        #print(f\"num_inputs2: {self.input_size}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "\n",
        "        self.critic = Critic(input_size=self.input_size,\n",
        "                             hidden_size=hidden_size\n",
        "                             )\n",
        "\n",
        "        self.actor = Actor(input_size=self.input_size,\n",
        "                           env_ids=env_ids,\n",
        "                           env_outputs=[self.env1_output, self.env2_output ],\n",
        "                           hidden_size=hidden_size\n",
        "                           )\n",
        "\n",
        "        self.log_std_env1 = nn.Parameter(torch.ones(1, self.env1_output) * std)\n",
        "        self.log_std_env2 = nn.Parameter(torch.ones(1, self.env2_output) * std)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.encoder.encode(self, x).to(device)\n",
        "        model_input = self.create_model_input(state, env_id)\n",
        "\n",
        "        value = self.critic(model_input)\n",
        "        mu    = self.actor(model_input)\n",
        "\n",
        "        if env_id == self.env1_id:   std = self.log_std_env1.exp().expand_as(mu)\n",
        "        elif env_id == self.env2_id: std = self.log_std_env2.exp().expand_as(mu)\n",
        "\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value\n",
        "\n",
        "    def create_model_input(self, state, env_id):\n",
        "         return {'state':state, 'env_id': env_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Losses**"
      ],
      "metadata": {
        "id": "TU6cXek1U_M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kldiv_loss = nn.KLDivLoss(reduction=\"batchmean\")"
      ],
      "metadata": {
        "id": "cER1ygQdVME8"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mku5EnhMtdBs"
      },
      "source": [
        "#Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "3ehz4izJtdBt"
      },
      "outputs": [],
      "source": [
        "def ppo_iter(hidden_network, mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states['network_PC'].size(0)\n",
        "    mini_batch_states    = {}\n",
        "    mini_batch_actions   = {}\n",
        "    mini_batch_log_probs = {}\n",
        "    mini_batch_returns   = {}\n",
        "    mini_batch_advantage = {}\n",
        "    for net in hidden_network.keys():\n",
        "            mini_batch_states[net]    = []\n",
        "            mini_batch_actions[net]   = []\n",
        "            mini_batch_log_probs[net] = []\n",
        "            mini_batch_returns[net]   = []\n",
        "            mini_batch_advantage[net] = []\n",
        "\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        for net in hidden_network.keys():\n",
        "            mini_batch_states[net].append(states[net][rand_ids, :])\n",
        "            mini_batch_actions[net].append(actions[net][rand_ids, :])\n",
        "            mini_batch_log_probs[net].append(log_probs[net][rand_ids, :])\n",
        "            mini_batch_returns[net].append(returns[net][rand_ids, :])\n",
        "            mini_batch_advantage[net].append(advantage[net][rand_ids, :])\n",
        "\n",
        "    ret_dict = {'states': mini_batch_states,\n",
        "                'actions': mini_batch_actions,\n",
        "                'log_probs':mini_batch_log_probs,\n",
        "                'returns': mini_batch_returns,\n",
        "                'advantage': mini_batch_advantage\n",
        "                }\n",
        "\n",
        "    return ret_dict, batch_size\n",
        "\n",
        "def ppo_update(hidden_network, env_id, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2, beta=0.5, omega=4, omega12=1):\n",
        "    for _ in range(ppo_epochs):\n",
        "        ret_dict, batch_size = ppo_iter(hidden_network, mini_batch_size, states, actions, log_probs, returns, advantages)\n",
        "        mini_batch_states = ret_dict['states']\n",
        "        mini_batch_actions = ret_dict['actions']\n",
        "        mini_batch_old_log_probs = ret_dict['log_probs']\n",
        "        mini_batch_returns = ret_dict['returns']\n",
        "        mini_batch_advantage = ret_dict['advantage']\n",
        "        for idx in range(batch_size // mini_batch_size):\n",
        "            new_log_probs = {}\n",
        "            model_input   = {}\n",
        "            dist          = {}\n",
        "            value         = {}\n",
        "            ppo_loss = 0\n",
        "            casc_loss = 0\n",
        "            net_id = 'network'\n",
        "            for i in range(cascade_nets):\n",
        "                if i == 0: NET = net_id + '_PC'\n",
        "                else: NET = net_id + '_' + str(i+1)\n",
        "\n",
        "                model_input[NET] = hidden_network[NET].create_model_input(np.array(mini_batch_states['network_PC'][idx]), env_id)\n",
        "                dist[NET], value[NET] = hidden_network[NET](model_input[NET])\n",
        "                new_log_probs[NET] = dist[NET].log_prob(mini_batch_actions[NET][idx])\n",
        "                entropy[NET] += dist[NET].entropy().mean()\n",
        "\n",
        "                ppo_loss += -beta*(omega**(i))*kldiv_loss(mini_batch_old_log_probs[NET][idx], new_log_probs[NET].exp())\n",
        "\n",
        "                if i == 0:\n",
        "                    casc_loss += -(omega12*kldiv_loss(mini_batch_old_log_probs[net_id + '_' + str(i+2)], new_log_probs[NET].exp()))\n",
        "                elif i == cascade_nets-1:\n",
        "                    casc_loss += -(omega*kldiv_loss(mini_batch_old_log_probs[net_id + '_' + str(i)][idx], new_log_probs[NET].exp())+kldiv_loss(new_log_probs[NET], new_log_probs[NET].exp()) )\n",
        "                else:\n",
        "                    casc_loss += -(omega*kldiv_loss(mini_batch_old_log_probs[net_id + '_' + str(i)][idx], new_log_probs[NET].exp())+kldiv_loss(mini_batch_old_log_probs[net_id + '_' + str(i+2)][idx], new_log_probs[NET].exp()) )\n",
        "\n",
        "            #model_input = model.create_model_input(state, env_id)\n",
        "            #dist, value = model(model_input)\n",
        "            #entropy = dist.entropy().mean()\n",
        "            #new_log_probs = dist.log_prob(action)\n",
        "\n",
        "\n",
        "            ratio = (new_log_probs['network_PC'] - mini_batch_old_log_probs['network_PC'][idx]).exp()\n",
        "            surr1 = ratio * mini_batch_advantage['network_PC'][idx]\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * mini_batch_advantage['network_PC'][idx]\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (mini_batch_returns['network_PC'][idx] - value['network_PC']).pow(2).mean()\n",
        "\n",
        "            pg_loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy['network_PC']\n",
        "\n",
        "            total_loss = ppo_loss.mean() + casc_loss.mean() + pg_loss\n",
        "\n",
        "            for net in hidden_network.keys():\n",
        "                hidden_network[net].optimizer.zero_grad()\n",
        "\n",
        "            total_loss.backward()\n",
        "\n",
        "            for net in hidden_network.keys():\n",
        "                hidden_network[net].optimizer.step()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZRZqea-qeB"
      },
      "source": [
        "#Main program"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialization for training"
      ],
      "metadata": {
        "id": "n6oG4o52CyUV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "4KLi-3vI-qeC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a405a018-7726-4169-9296-9158e98b349e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_inputs1: 3, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 2, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n"
          ]
        }
      ],
      "source": [
        "num_envs = 16\n",
        "\n",
        "env1_id = \"Pendulum-v1\"\n",
        "env2_id = \"MountainCarContinuous-v0\"\n",
        "env_ids = [env1_id, env2_id ]\n",
        "\n",
        "env1 = gym.make(env1_id)\n",
        "env2 = gym.make(env2_id)\n",
        "single_envs = { env1_id: env1,\n",
        "                env2_id: env2\n",
        "               }\n",
        "\n",
        "num_inputs_1  = env1.observation_space.shape[0];    num_outputs_1 = env1.action_space.shape[0]\n",
        "num_inputs_2  = env2.observation_space.shape[0];    num_outputs_2 = env2.action_space.shape[0]\n",
        "\n",
        "print(f\"num_inputs1: {num_inputs_1}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "print(f\"num_inputs2: {num_inputs_2}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "envs1 = make_vec_env(env1_id, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_id, n_envs=num_envs)\n",
        "\n",
        "vectorized_envs = { env1_id: envs1,\n",
        "                    env2_id: envs2\n",
        "                    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "DPBetpLoC3sa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "BPsNLqs9-qeD"
      },
      "outputs": [],
      "source": [
        "def swtich_enviroment( vectorized_envs, env_ids, env_index):\n",
        "    env_index += 1\n",
        "    new_id_env = env_ids[ env_index % 2 ]   # -------- the switch is only for the first two environments\n",
        "    new_env = vectorized_envs[ new_id_env ]\n",
        "    return new_env, new_id_env, env_index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(hidden_network, main_env, vect_envs, train_id_env, state, max_frames, test_rewards, f_idx):\n",
        "    frame_idx = f_idx\n",
        "    while frame_idx < max_frames and not early_stop:\n",
        "        print(f\"\\rit: {frame_idx}\", end=\"\")\n",
        "        log_probs = []\n",
        "        values    = []\n",
        "        states    = []\n",
        "        actions   = []\n",
        "        rewards   = []\n",
        "        masks     = []\n",
        "        entropy = 0\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "\n",
        "            model_input = main_model.create_model_input(state, train_id_env)\n",
        "            dist, value = main_model(model_input)\n",
        "\n",
        "            action = dist.sample()\n",
        "            next_state, reward, done, _ = vect_envs.step(action.cpu().numpy())\n",
        "\n",
        "            log_prob = dist.log_prob(action)\n",
        "            entropy += dist.entropy().mean()\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "            masks.append(torch.FloatTensor(1-done).unsqueeze(1).to(device))\n",
        "\n",
        "            states.append(torch.FloatTensor(model_input['state'] ).to(device) )\n",
        "            actions.append(action)\n",
        "\n",
        "            state = next_state\n",
        "            frame_idx += 1\n",
        "\n",
        "            if frame_idx % plot_frequency == 0:\n",
        "                test_reward = np.mean([test_env(main_model, main_env, train_id_env) for _ in range(10)])\n",
        "                test_rewards.append(test_reward)\n",
        "                plot(train_id_env,frame_idx, test_rewards) # we cant use earlystop with env Mountaincar\n",
        "\n",
        "\n",
        "        model_input = main_model.create_model_input(state, train_id_env)\n",
        "        _, next_value = main_model(model_input)\n",
        "\n",
        "        returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "        returns   = torch.cat(returns).detach()\n",
        "        log_probs = torch.cat(log_probs).detach()\n",
        "        values    = torch.cat(values).detach()\n",
        "        states    = torch.cat(states)\n",
        "        actions   = torch.cat(actions)\n",
        "        advantage = returns - values\n",
        "\n",
        "\n",
        "        ppo_update(main_model, optimizer, train_id_env, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
      ],
      "metadata": {
        "id": "XUug4y3KCNMX"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Settings an dHyperparameters**"
      ],
      "metadata": {
        "id": "U00zUq-rsQgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyper params:\n",
        "hidden_size      = 256\n",
        "lr               = 3e-4\n",
        "num_steps        = 20\n",
        "mini_batch_size  = 5\n",
        "ppo_epochs       = 4\n",
        "threshold_reward = 0\n",
        "\n",
        "epoch = 18\n",
        "train_per_epoch = 30000\n",
        "\n",
        "swtich_counter = 0\n",
        "switch_env_frequency = 3\n",
        "\n",
        "cascade_nets = 4\n",
        "hidden_network = {}\n",
        "base_id = 'network'\n",
        "\n",
        "for i in range(cascade_nets):\n",
        "    if i == 0: hidden_id = base_id + '_PC'\n",
        "    else: hidden_id = base_id + '_' + str(i+1)\n",
        "    hidden_network[hidden_id] = ActorCritic(envs=vectorized_envs, env_ids=env_ids, hidden_size=256, lr=lr).to(device)\n",
        "print(hidden_network.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZhooBE5sftX",
        "outputId": "0f3d7484-2ef5-48b7-eac5-e8c51865b3bc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['network_PC', 'network_2', 'network_3', 'network_4'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inizio Training**"
      ],
      "metadata": {
        "id": "TxvF-J2nFmH1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "t-LEJNaxtdBu",
        "outputId": "5ebe6766-4db1-4f46-fa64-1ed6c91b3be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 0, actual_id_env: MountainCarContinuous-v0, env_index: 1, switch: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "kl_div(): argument 'input' (position 1) must be Tensor, not list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-178d67c9b497>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mppo_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_id_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppo_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-94d183de3df5>\u001b[0m in \u001b[0;36mppo_update\u001b[0;34m(hidden_network, env_id, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param, beta, omega, omega12)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0mcasc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momega12\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkldiv_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_old_log_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_log_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcascade_nets\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0mcasc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momega\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkldiv_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_old_log_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_log_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mkldiv_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_log_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_log_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mkl_div\u001b[0;34m(input, target, size_average, reduce, reduction, log_target)\u001b[0m\n\u001b[1;32m   2953\u001b[0m             \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2955\u001b[0;31m     \u001b[0mreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batchmean\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: kl_div(): argument 'input' (position 1) must be Tensor, not list"
          ]
        }
      ],
      "source": [
        "#early_stop = False\n",
        "test_rewards = []\n",
        "\n",
        "env_states = {}     # contain state to pass when callicng act\n",
        "env_steps  = {}     # counter for steps\n",
        "env_reward = {}     # counter for reward\n",
        "env_index  = 1\n",
        "\n",
        "# Reset of environments and initializing states\n",
        "for id_env in vectorized_envs.keys():\n",
        "    env_states[id_env] = vectorized_envs[id_env].reset()\n",
        "    env_reward[id_env] = []\n",
        "    env_steps[id_env]   = 0\n",
        "\n",
        "actual_id_env = env_ids[env_index]\n",
        "actual_vectorized_env = vectorized_envs[actual_id_env]\n",
        "actual_single_env = single_envs[actual_id_env]\n",
        "\n",
        "#optimizer = optim.Adam(model_PC.parameters(), lr=lr)  #: ----> Optimizer declared inside class ActorCritic()\n",
        "\n",
        "for e in range(epoch):\n",
        "    if e > 1 and ( e %  switch_env_frequency == 0):\n",
        "        #print(\"entered\")\n",
        "        actual_vectorized_env, actual_id_env, env_index = swtich_enviroment(vectorized_envs, env_ids, env_index)\n",
        "        actual_single_env=single_envs[actual_id_env]\n",
        "        swtich_counter += 1\n",
        "\n",
        "    print(f\"e: {e}, actual_id_env: {actual_id_env}, env_index: {env_index}, switch: {swtich_counter}\")\n",
        "\n",
        "    actual_state = env_states[actual_id_env]\n",
        "    frame_idx = 0\n",
        "    while frame_idx < train_per_epoch:\n",
        "        log_probs   = {}\n",
        "        values      = {}\n",
        "        states      = {}\n",
        "        actions     = {}\n",
        "        rewards     = {}\n",
        "        masks       = {}\n",
        "        model_input = {}\n",
        "        dist        = {}\n",
        "        value       = {}\n",
        "        action      = {}\n",
        "        next_state  = {}\n",
        "        reward      = {}\n",
        "        done        = {}\n",
        "        log_prob    = {}\n",
        "        entropy     = {}\n",
        "        next_value  = {}\n",
        "        returns     = {}\n",
        "        advantage   = {}\n",
        "\n",
        "        for net in hidden_network.keys():\n",
        "            entropy[net]   = 0\n",
        "            log_probs[net] = []\n",
        "            values[net]    = []\n",
        "            states[net]    = []\n",
        "            actions[net]   = []\n",
        "            rewards[net]   = []\n",
        "            masks[net]     = []\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "\n",
        "            for net in hidden_network.keys():\n",
        "              state = env_states[actual_id_env]\n",
        "              model_input[net] = hidden_network[net].create_model_input(state, actual_id_env)\n",
        "              dist[net], value[net] = hidden_network[net](model_input[net])\n",
        "\n",
        "              action[net] = dist[net].sample()\n",
        "              next_state[net], reward[net], done[net], _ = actual_vectorized_env.step(action[net].cpu().numpy())\n",
        "              #print(f\"\\nterminated: {terminated}, truncated: {truncated}\")\n",
        "              #done[net] = [terminated[i] or truncated[i]['TimeLimit.truncated'] for i in range(num_envs)]\n",
        "              #print(f\"\\ndone[net]: {done[net]}\")\n",
        "\n",
        "              log_prob[net] = dist[net].log_prob(action[net])\n",
        "              entropy[net] += dist[net].entropy().mean()\n",
        "\n",
        "              log_probs[net].append(log_prob[net])\n",
        "              values[net].append(value[net])\n",
        "              rewards[net].append(torch.FloatTensor(reward[net]).unsqueeze(1).to(device))\n",
        "              masks[net].append(torch.FloatTensor(1-done[net]).unsqueeze(1).to(device))\n",
        "\n",
        "              states[net].append(torch.FloatTensor(model_input[net]['state'] ).to(device) )\n",
        "              actions[net].append(action[net])\n",
        "\n",
        "            #model_input = main_model.create_model_input(state, train_id_env)\n",
        "            #dist, value = main_model(model_input)\n",
        "\n",
        "            #action = dist.sample()\n",
        "            #next_state, reward, done, _ = vect_envs.step(action.cpu().numpy())\n",
        "\n",
        "            #log_prob = dist.log_prob(action)\n",
        "            #entropy += dist.entropy().mean()\n",
        "\n",
        "            #log_probs.append(log_prob)\n",
        "            #values.append(value)\n",
        "            #rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "            #masks.append(torch.FloatTensor(1-done).unsqueeze(1).to(device))\n",
        "\n",
        "            #states.append(torch.FloatTensor(model_input['state'] ).to(device) )\n",
        "            #actions.append(action)\n",
        "\n",
        "            env_states[actual_id_env] = next_state['network_PC']\n",
        "            frame_idx += 1\n",
        "\n",
        "            if frame_idx % 1000 == 0:\n",
        "                test_reward = np.mean([test_env(hidden_network['network_PC'], actual_single_env, actual_id_env) for _ in range(10)])\n",
        "                test_rewards.append(test_reward)\n",
        "                plot(actual_id_env,frame_idx + e*train_per_epoch, test_rewards)\n",
        "                # we cant use earlystop with env Mountaincar\n",
        "\n",
        "        for net in hidden_network.keys():\n",
        "          model_input[net] = hidden_network[net].create_model_input(state, actual_id_env)\n",
        "          _, next_value[net] = hidden_network[net](model_input[net])\n",
        "\n",
        "          returns[net] = compute_gae(next_value[net], rewards[net], masks[net], values[net])\n",
        "\n",
        "          returns[net]   = torch.cat(returns[net]).detach()\n",
        "          log_probs[net] = torch.cat(log_probs[net]).detach()\n",
        "          values[net]    = torch.cat(values[net]).detach()\n",
        "          states[net]    = torch.cat(states[net])\n",
        "          actions[net]   = torch.cat(actions[net])\n",
        "          advantage[net] = returns[net] - values[net]\n",
        "\n",
        "        #model_input = main_model.create_model_input(state, train_id_env)\n",
        "        #_, next_value = main_model(model_input)\n",
        "\n",
        "        #returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "        #returns   = torch.cat(returns).detach()\n",
        "        #log_probs = torch.cat(log_probs).detach()\n",
        "        #values    = torch.cat(values).detach()\n",
        "        #states    = torch.cat(states)\n",
        "        #actions   = torch.cat(actions)\n",
        "        #advantage = returns - values\n",
        "\n",
        "\n",
        "        ppo_update(hidden_network, actual_id_env, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iUo1aLitdBv"
      },
      "source": [
        "#Saving trajectories for GAIL - delete ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15oqasvitdBv"
      },
      "outputs": [],
      "source": [
        "from itertools import count\n",
        "\n",
        "max_expert_num = 50000\n",
        "num_steps = 0\n",
        "expert_traj = []\n",
        "\n",
        "for i_episode in count():\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        action = dist.sample().cpu().numpy()[0]\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        expert_traj.append(np.hstack([state, action]))\n",
        "        num_steps += 1\n",
        "\n",
        "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
        "\n",
        "    if num_steps >= max_expert_num:\n",
        "        break\n",
        "\n",
        "expert_traj = np.stack(expert_traj)\n",
        "print()\n",
        "print(expert_traj.shape)\n",
        "print()\n",
        "np.save(\"expert_traj.npy\", expert_traj)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Z8L3VZyPtdBq",
        "DPBetpLoC3sa"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}