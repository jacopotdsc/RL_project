{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Initialization"
      ],
      "metadata": {
        "id": "zSHiaIsp_hxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "ccdKZ4CltkW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d2ca59-2e01-43b1-9ea2-77c5bd740bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "! pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "DmPSS_NatdBk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn.preprocessing\n",
        "from sklearn.kernel_approximation import RBFSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "Refo9Y_vtdBn"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "Eq5dx6SGtdBp"
      },
      "outputs": [],
      "source": [
        "samples = [[-0.6810, -0.7323, -0.3519],\n",
        "        [ 0.9754, -0.2205, -0.9598],\n",
        "        [ 0.8309, -0.5564, -0.8232],\n",
        "        [ 0.4757,  0.8796,  0.8216],\n",
        "        [-0.9221, -0.3869,  0.8180],\n",
        "        [-0.9108, -0.4129,  0.2524],\n",
        "        [-0.1120, -0.9937, -0.8391],\n",
        "        [ 0.5760,  0.8175,  0.6597],\n",
        "        [ 0.4632, -0.8863, -0.1590],\n",
        "        [ 0.5347,  0.8450, -0.4712],\n",
        "        [ 0.5242,  0.8516, -0.9269],\n",
        "        [-0.8494, -0.5277,  0.9623],\n",
        "        [ 0.8243, -0.5661, -0.8888],\n",
        "        [-0.9457, -0.3252, -0.5394],\n",
        "        [ 0.9998, -0.0188, -0.7901],\n",
        "        [-0.9918, -0.1282,  0.9201]]\n",
        "\n",
        "num_envs = 16\n",
        "\n",
        "env1_name = \"Pendulum-v1\"\n",
        "env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "env1 = gym.make(env1_name)\n",
        "env2 = gym.make(env2_name)\n",
        "\n",
        "envs1 = make_vec_env(env1_name, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_name, n_envs=num_envs)\n",
        "\n",
        "p_envs_ids = [env1_name, env2_name ]\n",
        "p_envs = {env1_name: envs1,\n",
        "          env2_name: envs2\n",
        "          }\n",
        "\n",
        "#model_ac = ActorCritic(envs=p_envs, env_ids=p_envs_ids, hidden_size=256).to(device)\n",
        "#model_input = model_ac.create_model_input(samples, env1_name)\n",
        "\n",
        "#dist, value = model_ac(model_input)\n",
        "#print(f\"dist: {dist}, val: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Auxiliar function"
      ],
      "metadata": {
        "id": "Fy_Ly0wC_Yiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "zbZ12TwvtdBr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "980958d5-71cd-4efb-e0c4-4bb2db688f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:246: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot select an axis to squeeze out which has size not equal to one",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-349-06e57a288626>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_PC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv1_name\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_PC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv2_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-349-06e57a288626>\u001b[0m in \u001b[0;36mtest_env\u001b[0;34m(model, env, id_env, vis)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(f\"sq: {[next_state.squeeze(-1)]}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
          ]
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "def plot(name, frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title(name + ': ' + 'frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "\n",
        "def test_env(model, env, id_env, vis=False):\n",
        "\n",
        "    state = torch.FloatTensor(env.reset()[0]).unsqueeze(0)\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        #state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        #print(state)\n",
        "        #state = torch.FloatTensor(state).to(device)\n",
        "        #dist, _ = model(model_input)\n",
        "        state = np.array(state)\n",
        "        model_input = model.create_model_input(state, id_env)\n",
        "        dist, _ = model(model_input)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(dist.sample().cpu().numpy())\n",
        "        done = terminated or truncated\n",
        "\n",
        "\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        state = np.expand_dims( np.array(next_state.squeeze(-1)), 0 )\n",
        "        #print(f\"sq: {[next_state.squeeze(-1)]}\")\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "env1_name = \"Pendulum-v1\"\n",
        "env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "env1 = gym.make(env1_name)\n",
        "env2 = gym.make(env2_name)\n",
        "\n",
        "#test_env(model_PC, env1, env1_name )\n",
        "#test_env(model_PC, env2, env2_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "5pIDF5hftdBs"
      },
      "outputs": [],
      "source": [
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8L3VZyPtdBq"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "lcolBdfL-qd2"
      },
      "outputs": [],
      "source": [
        "class RBFFeatureEncoder:\n",
        "    def __init__(self, env1, env2, env3, n_component=100):\n",
        "\n",
        "        self.env1 = env1\n",
        "        self.env2 = env2\n",
        "        self.env3 = env3\n",
        "\n",
        "        data1 = np.array([env1.observation_space.sample() for x in range(10000)]); data1 = np.clip(data1, -1e38, 1e38)\n",
        "        data2 = np.array([env2.observation_space.sample() for x in range(10000)]); data2 = np.clip(data2, -1e38, 1e38)\n",
        "        data3 = np.array([env3.observation_space.sample() for x in range(10000)]); data3 = np.clip(data3, -1e38, 1e38)\n",
        "\n",
        "        self.rbf_sampler1 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler2 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler3 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "\n",
        "        self.standard_scaler1 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler2 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler3 = sklearn.preprocessing.StandardScaler()\n",
        "\n",
        "        self.standard_scaler1.fit(data1)\n",
        "        self.standard_scaler2.fit(data2)\n",
        "        self.standard_scaler3.fit(data3)\n",
        "\n",
        "        transformed_data1 = self.standard_scaler1.transform(data1)\n",
        "        transformed_data2 = self.standard_scaler2.transform(data2)\n",
        "        transformed_data3 = self.standard_scaler3.transform(data3)\n",
        "\n",
        "        self.rbf_sampler1.fit( transformed_data1 )\n",
        "        self.rbf_sampler2.fit( transformed_data2 )\n",
        "        self.rbf_sampler3.fit( transformed_data3 )\n",
        "\n",
        "    def wrapper_encode(self, model, model_input):\n",
        "\n",
        "        state = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "\n",
        "        if model.env1_id == env_id:\n",
        "            transformed_state = self.standard_scaler1.transform([state])\n",
        "            encoded_state = self.rbf_sampler1.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env2_id == env_id:\n",
        "            transformed_state = self.standard_scaler2.transform([state])\n",
        "            encoded_state = self.rbf_sampler2.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env3_id == env_id:\n",
        "            transformed_state = self.standard_scaler3.transform([state])\n",
        "            encoded_state = self.rbf_sampler3.transform(transformed_state).flatten()\n",
        "\n",
        "        return encoded_state\n",
        "\n",
        "    def encode(self, model, model_input):\n",
        "\n",
        "        states = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "        encoded_states = []\n",
        "\n",
        "        for s in states:\n",
        "            new_input = model.create_model_input(s,env_id )\n",
        "            encoded_states.append( self.wrapper_encode(model, new_input) )\n",
        "\n",
        "        return torch.tensor(encoded_states, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self.rbf_sampler1.n_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "1-VdeX7z-qd4"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "        state = self.output_layer(state)\n",
        "        return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "ZZfIK7Nu-qd5"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_size, env_outputs, hidden_size, env_ids):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "        self.env1_actions = env_outputs[0]\n",
        "        self.env2_actions = env_outputs[1]\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        self.env1_output = nn.Linear(hidden_size, self.env1_actions)\n",
        "        self.env2_output = nn.Linear(hidden_size, self.env2_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "\n",
        "        if env_id == self.env1_id:   mu = self.env1_output(state)\n",
        "        elif env_id == self.env2_id: mu = self.env2_output(state)\n",
        "\n",
        "        return mu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "llFUdL-x-qd6"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, envs, env_ids, hidden_size, std=0.0, lr=0.0001):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "\n",
        "        self.envs1   = envs[self.env1_id]\n",
        "        self.envs2   = envs[self.env2_id]\n",
        "\n",
        "        self.env1_output = self.envs1.action_space.shape[0]\n",
        "        self.env2_output = self.envs2.action_space.shape[0]\n",
        "\n",
        "        self.encoder    = RBFFeatureEncoder(self.envs1, self.envs2, self.envs2)\n",
        "        self.input_size = self.encoder.size\n",
        "\n",
        "        print(f\"num_inputs1: {self.input_size}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "        print(f\"num_inputs2: {self.input_size}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "\n",
        "        self.critic = Critic(input_size=self.input_size,\n",
        "                             hidden_size=hidden_size\n",
        "                             )\n",
        "\n",
        "        self.actor = Actor(input_size=self.input_size,\n",
        "                           env_ids=env_ids,\n",
        "                           env_outputs=[self.env1_output, self.env2_output ],\n",
        "                           hidden_size=hidden_size\n",
        "                           )\n",
        "\n",
        "        self.log_std_env1 = nn.Parameter(torch.ones(1, self.env1_output) * std)\n",
        "        self.log_std_env2 = nn.Parameter(torch.ones(1, self.env2_output) * std)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.encoder.encode(self, x).to(device)\n",
        "        model_input = self.create_model_input(state, env_id)\n",
        "\n",
        "        value = self.critic(model_input)\n",
        "        mu    = self.actor(model_input)\n",
        "\n",
        "        if env_id == self.env1_id:   std = self.log_std_env1.exp().expand_as(mu)\n",
        "        elif env_id == self.env2_id: std = self.log_std_env2.exp().expand_as(mu)\n",
        "\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value\n",
        "\n",
        "    def create_model_input(self, state, env_id):\n",
        "         return {'state':state, 'env_id': env_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mku5EnhMtdBs"
      },
      "source": [
        "#Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "3ehz4izJtdBt"
      },
      "outputs": [],
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
        "\n",
        "def ppo_update(model, optimizer, env_id, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "\n",
        "            state = np.array(state)\n",
        "            model_input = model.create_model_input(state, env_id)\n",
        "            dist, value = model(model_input)\n",
        "            #dist, value = model(state)\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            ratio = (new_log_probs - old_log_probs).exp()\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZRZqea-qeB"
      },
      "source": [
        "#Main program"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialization for training"
      ],
      "metadata": {
        "id": "n6oG4o52CyUV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "4KLi-3vI-qeC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd32ee0-f47a-436b-deef-3ff2e708e41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_inputs1: 3, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 2, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n"
          ]
        }
      ],
      "source": [
        "num_envs = 16\n",
        "\n",
        "env1_id = \"Pendulum-v1\"\n",
        "env2_id = \"MountainCarContinuous-v0\"\n",
        "env_ids = [env1_id, env2_id ]\n",
        "\n",
        "env1 = gym.make(env1_id)\n",
        "env2 = gym.make(env2_id)\n",
        "single_envs = { env1_id: env1,\n",
        "                env2_id: env2\n",
        "               }\n",
        "\n",
        "num_inputs_1  = env1.observation_space.shape[0];    num_outputs_1 = env1.action_space.shape[0]\n",
        "num_inputs_2  = env2.observation_space.shape[0];    num_outputs_2 = env2.action_space.shape[0]\n",
        "\n",
        "print(f\"num_inputs1: {num_inputs_1}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "print(f\"num_inputs2: {num_inputs_2}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "envs1 = make_vec_env(env1_name, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_name, n_envs=num_envs)\n",
        "\n",
        "vectorized_envs = { env1_id: envs1,\n",
        "                    env2_id: envs2\n",
        "                    }\n",
        "\n",
        "#Hyper params:\n",
        "hidden_size      = 256\n",
        "lr               = 3e-4\n",
        "num_steps        = 20\n",
        "mini_batch_size  = 5\n",
        "ppo_epochs       = 4\n",
        "threshold_reward = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden_network = 4\n",
        "hidden_network = {}\n",
        "for i in range(4):\n",
        "    hidden_id = 'network'\n",
        "    if i == 0: hidden_id += '_PC'\n",
        "    else: hidden_id += '_' + str(i+1)\n",
        "\n",
        "    hidden_network[hidden_id] = ActorCritic(envs=vectorized_envs, env_ids=env_ids, hidden_size=256, lr=lr).to(device)\n",
        "\n",
        "print(hidden_network.keys())\n",
        "\n",
        "#model_PC = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)\n",
        "#model_2 = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)\n",
        "#model_3 = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)\n",
        "#model_4 = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)"
      ],
      "metadata": {
        "id": "vFAGCHGGA4tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "DPBetpLoC3sa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "BPsNLqs9-qeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7c87a3-d736-4a05-9604-818ac66d73ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def swtich_enviroment( vectorized_envs, env_ids, env_index):\n",
        "    env_index += 1\n",
        "    new_id_env = env_ids[ env_index % 2 ]   # -------- the switch is only for the first two environments\n",
        "    new_env = vectorized_envs[ new_id_env ]\n",
        "    return new_env, new_id_env, env_index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(main_model, main_env, optimizer, vect_envs, train_id_env, state, max_frames, early_stop, test_rewards, plot_frequency=1000):\n",
        "    frame_idx = 0\n",
        "    while frame_idx < max_frames and not early_stop:\n",
        "        print(f\"\\rit: {frame_idx}\", end=\"\")\n",
        "        log_probs = []\n",
        "        values    = []\n",
        "        states    = []\n",
        "        actions   = []\n",
        "        rewards   = []\n",
        "        masks     = []\n",
        "        entropy = 0\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "\n",
        "            model_input = main_model.create_model_input(state, train_id_env)\n",
        "            dist, value = main_model(model_input)\n",
        "\n",
        "            action = dist.sample()\n",
        "            next_state, reward, done, _ = vect_envs.step(action.cpu().numpy())\n",
        "\n",
        "            log_prob = dist.log_prob(action)\n",
        "            entropy += dist.entropy().mean()\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "            masks.append(torch.FloatTensor(1-done).unsqueeze(1).to(device))\n",
        "\n",
        "            states.append(torch.FloatTensor(model_input['state'] ).to(device) )\n",
        "            actions.append(action)\n",
        "\n",
        "            state = next_state\n",
        "            frame_idx += 1\n",
        "\n",
        "            if frame_idx % plot_frequency == 0:\n",
        "                test_reward = np.mean([test_env(main_model, main_env, train_id_env) for _ in range(10)])\n",
        "                test_rewards.append(test_reward)\n",
        "                plot(train_id_env,frame_idx, test_rewards)\n",
        "                if test_reward > threshold_reward: early_stop = True\n",
        "\n",
        "\n",
        "        model_input = main_model.create_model_input(state, train_id_env)\n",
        "        _, next_value = main_model(model_input)\n",
        "\n",
        "        returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "        returns   = torch.cat(returns).detach()\n",
        "        log_probs = torch.cat(log_probs).detach()\n",
        "        values    = torch.cat(values).detach()\n",
        "        states    = torch.cat(states)\n",
        "        actions   = torch.cat(actions)\n",
        "        advantage = returns - values\n",
        "\n",
        "\n",
        "        ppo_update(main_model, optimizer, train_id_env, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
      ],
      "metadata": {
        "id": "XUug4y3KCNMX"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "t-LEJNaxtdBu",
        "outputId": "19c14efb-7dfd-4793-c672-bdb13fc14372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e: 0, actual_id_env: MountainCarContinuous-v0, env_index: 1, switch: 0\n",
            "\rit: 0[-0.459477    0.00102963]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot select an axis to squeeze out which has size not equal to one",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-300-cd0cba6c2d31>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mactual_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactual_id_env\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     train(main_model=model_PC, \n\u001b[0m\u001b[1;32m     40\u001b[0m           \u001b[0mmain_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactual_single_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-288-74bc92ff354d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(main_model, main_env, optimizer, vect_envs, train_id_env, state, max_frames, early_stop, test_rewards, plot_frequency)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mplot_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mtest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_id_env\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mtest_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_id_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-288-74bc92ff354d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mplot_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mtest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_id_env\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mtest_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_id_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-299-49fac5071d73>\u001b[0m in \u001b[0;36mtest_env\u001b[0;34m(model, env, id_env, vis)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
          ]
        }
      ],
      "source": [
        "epoch = 20\n",
        "epoch_per_train = 30000\n",
        "\n",
        "swtich_counter = 0\n",
        "switch_env_frequency = 2\n",
        "\n",
        "early_stop = False\n",
        "#test_rewards = []\n",
        "\n",
        "\n",
        "env_states = {}     # contain state to pass when callicng act\n",
        "env_step   = {}     # counter for switch\n",
        "env_reward = {}     # counter for reward\n",
        "env_index  = 1\n",
        "\n",
        "\n",
        "for id_env in vectorized_envs.keys():\n",
        "    env_states[id_env] = vectorized_envs[id_env].reset()\n",
        "    env_reward[id_env] = []\n",
        "    env_step[id_env]   = 0\n",
        "\n",
        "actual_id_env = env_ids[env_index]\n",
        "actual_vectorized_env = vectorized_envs[actual_id_env]\n",
        "actual_single_env = single_envs[actual_id_env]\n",
        "\n",
        "model_PC = hidden_network['network_PC']\n",
        "optimizer = optim.Adam(model_PC.parameters(), lr=lr)\n",
        "\n",
        "for e in range(epoch):\n",
        "    if e > 1 and ( e %  switch_env_frequency == 0):\n",
        "        print(\"entered\")\n",
        "        actual_vectorized_env, actual_id_env, env_index = swtich_enviroment(vectorized_envs, env_ids, env_index)\n",
        "        actual_single_env=single_envs[actual_id_env]\n",
        "        swtich_counter += 1\n",
        "\n",
        "    print(f\"e: {e}, actual_id_env: {actual_id_env}, env_index: {env_index}, switch: {swtich_counter}\")\n",
        "\n",
        "    actual_state = env_states[actual_id_env]\n",
        "    train(main_model=model_PC,\n",
        "          main_env=actual_single_env,\n",
        "          optimizer=optimizer,\n",
        "          vect_envs=actual_vectorized_env,\n",
        "          train_id_env=actual_id_env,\n",
        "          state=actual_state,\n",
        "          max_frames=40, #epoch_per_train,\n",
        "          early_stop=early_stop,\n",
        "          test_rewards = env_reward[actual_id_env],\n",
        "          plot_frequency=10\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iUo1aLitdBv"
      },
      "source": [
        "#Saving trajectories for GAIL - delete ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15oqasvitdBv"
      },
      "outputs": [],
      "source": [
        "from itertools import count\n",
        "\n",
        "max_expert_num = 50000\n",
        "num_steps = 0\n",
        "expert_traj = []\n",
        "\n",
        "for i_episode in count():\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        action = dist.sample().cpu().numpy()[0]\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        expert_traj.append(np.hstack([state, action]))\n",
        "        num_steps += 1\n",
        "\n",
        "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
        "\n",
        "    if num_steps >= max_expert_num:\n",
        "        break\n",
        "\n",
        "expert_traj = np.stack(expert_traj)\n",
        "print()\n",
        "print(expert_traj.shape)\n",
        "print()\n",
        "np.save(\"expert_traj.npy\", expert_traj)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}