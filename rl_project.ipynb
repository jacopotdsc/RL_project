{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSHiaIsp_hxu"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ccdKZ4CltkW3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "DmPSS_NatdBk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn.preprocessing\n",
        "from sklearn.kernel_approximation import RBFSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Refo9Y_vtdBn"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Eq5dx6SGtdBp"
      },
      "outputs": [],
      "source": [
        "samples = [[-0.6810, -0.7323, -0.3519],\n",
        "        [ 0.9754, -0.2205, -0.9598],\n",
        "        [ 0.8309, -0.5564, -0.8232],\n",
        "        [ 0.4757,  0.8796,  0.8216],\n",
        "        [-0.9221, -0.3869,  0.8180],\n",
        "        [-0.9108, -0.4129,  0.2524],\n",
        "        [-0.1120, -0.9937, -0.8391],\n",
        "        [ 0.5760,  0.8175,  0.6597],\n",
        "        [ 0.4632, -0.8863, -0.1590],\n",
        "        [ 0.5347,  0.8450, -0.4712],\n",
        "        [ 0.5242,  0.8516, -0.9269],\n",
        "        [-0.8494, -0.5277,  0.9623],\n",
        "        [ 0.8243, -0.5661, -0.8888],\n",
        "        [-0.9457, -0.3252, -0.5394],\n",
        "        [ 0.9998, -0.0188, -0.7901],\n",
        "        [-0.9918, -0.1282,  0.9201]]\n",
        "\n",
        "num_envs = 16\n",
        "\n",
        "env1_name = \"Pendulum-v1\"\n",
        "env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "env1 = gym.make(env1_name)\n",
        "env2 = gym.make(env2_name)\n",
        "\n",
        "envs1 = make_vec_env(env1_name, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_name, n_envs=num_envs)\n",
        "\n",
        "p_envs_ids = [env1_name, env2_name ]\n",
        "p_envs = {env1_name: envs1,\n",
        "          env2_name: envs2\n",
        "          }\n",
        "\n",
        "#model_ac = ActorCritic(envs=p_envs, env_ids=p_envs_ids, hidden_size=256).to(device)\n",
        "#model_input = model_ac.create_model_input(samples, env1_name)\n",
        "\n",
        "#dist, value = model_ac(model_input)\n",
        "#print(f\"dist: {dist}, val: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy_Ly0wC_Yiv"
      },
      "source": [
        "#Auxiliar function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "zbZ12TwvtdBr"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "def plot(name, frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title(name + ': ' + 'frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "\n",
        "def test_env(model, env, id_env, vis=False):\n",
        "\n",
        "    state = torch.FloatTensor(env.reset()[0]).unsqueeze(0)\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        #state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        #print(state)\n",
        "        #state = torch.FloatTensor(state).to(device)\n",
        "        #dist, _ = model(model_input)\n",
        "        state = np.array(state)\n",
        "        model_input = model.create_model_input(state, id_env)\n",
        "        dist, _ = model(model_input)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(dist.sample().cpu().numpy())\n",
        "        done = terminated or truncated\n",
        "\n",
        "\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        state = np.expand_dims( np.array(next_state.squeeze(-1)), 0 )\n",
        "        #print(f\"sq: {[next_state.squeeze(-1)]}\")\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "env1_name = \"Pendulum-v1\"\n",
        "env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "env1 = gym.make(env1_name)\n",
        "env2 = gym.make(env2_name)\n",
        "\n",
        "#test_env(model_PC, env1, env1_name )\n",
        "#test_env(model_PC, env2, env2_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8L3VZyPtdBq"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "lcolBdfL-qd2"
      },
      "outputs": [],
      "source": [
        "class RBFFeatureEncoder:\n",
        "    def __init__(self, env1, env2, env3, n_component=100):\n",
        "\n",
        "        self.env1 = env1\n",
        "        self.env2 = env2\n",
        "        self.env3 = env3\n",
        "\n",
        "        data1 = np.array([env1.observation_space.sample() for x in range(10000)]); data1 = np.clip(data1, -1e38, 1e38)\n",
        "        data2 = np.array([env2.observation_space.sample() for x in range(10000)]); data2 = np.clip(data2, -1e38, 1e38)\n",
        "        data3 = np.array([env3.observation_space.sample() for x in range(10000)]); data3 = np.clip(data3, -1e38, 1e38)\n",
        "\n",
        "        self.rbf_sampler1 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler2 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler3 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "\n",
        "        self.standard_scaler1 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler2 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler3 = sklearn.preprocessing.StandardScaler()\n",
        "\n",
        "        self.standard_scaler1.fit(data1)\n",
        "        self.standard_scaler2.fit(data2)\n",
        "        self.standard_scaler3.fit(data3)\n",
        "\n",
        "        transformed_data1 = self.standard_scaler1.transform(data1)\n",
        "        transformed_data2 = self.standard_scaler2.transform(data2)\n",
        "        transformed_data3 = self.standard_scaler3.transform(data3)\n",
        "\n",
        "        self.rbf_sampler1.fit( transformed_data1 )\n",
        "        self.rbf_sampler2.fit( transformed_data2 )\n",
        "        self.rbf_sampler3.fit( transformed_data3 )\n",
        "\n",
        "    def wrapper_encode(self, model, model_input):\n",
        "\n",
        "        state = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "\n",
        "        if model.env1_id == env_id:\n",
        "            transformed_state = self.standard_scaler1.transform([state])\n",
        "            encoded_state = self.rbf_sampler1.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env2_id == env_id:\n",
        "            transformed_state = self.standard_scaler2.transform([state])\n",
        "            encoded_state = self.rbf_sampler2.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env3_id == env_id:\n",
        "            transformed_state = self.standard_scaler3.transform([state])\n",
        "            encoded_state = self.rbf_sampler3.transform(transformed_state).flatten()\n",
        "\n",
        "        return encoded_state\n",
        "\n",
        "    def encode(self, model, model_input):\n",
        "\n",
        "        states = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "        encoded_states = []\n",
        "\n",
        "        for s in states:\n",
        "            new_input = model.create_model_input(s,env_id )\n",
        "            encoded_states.append( self.wrapper_encode(model, new_input) )\n",
        "\n",
        "        return torch.tensor(encoded_states, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self.rbf_sampler1.n_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "1-VdeX7z-qd4"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "        state = self.output_layer(state)\n",
        "        return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ZZfIK7Nu-qd5"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_size, env_outputs, hidden_size, env_ids):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "        self.env1_actions = env_outputs[0]\n",
        "        self.env2_actions = env_outputs[1]\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        self.env1_output = nn.Linear(hidden_size, self.env1_actions)\n",
        "        self.env2_output = nn.Linear(hidden_size, self.env2_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "\n",
        "        if env_id == self.env1_id:   mu = self.env1_output(state)\n",
        "        elif env_id == self.env2_id: mu = self.env2_output(state)\n",
        "\n",
        "        return mu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "llFUdL-x-qd6"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, envs, env_ids, hidden_size, std=0.0, lr=0.0001):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "\n",
        "        self.envs1   = envs[self.env1_id]\n",
        "        self.envs2   = envs[self.env2_id]\n",
        "\n",
        "        self.env1_output = self.envs1.action_space.shape[0]\n",
        "        self.env2_output = self.envs2.action_space.shape[0]\n",
        "\n",
        "        self.encoder    = RBFFeatureEncoder(self.envs1, self.envs2, self.envs2)\n",
        "        self.input_size = self.encoder.size\n",
        "\n",
        "        print(f\"num_inputs1: {self.input_size}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "        print(f\"num_inputs2: {self.input_size}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "\n",
        "        self.critic = Critic(input_size=self.input_size,\n",
        "                             hidden_size=hidden_size\n",
        "                             )\n",
        "\n",
        "        self.actor = Actor(input_size=self.input_size,\n",
        "                           env_ids=env_ids,\n",
        "                           env_outputs=[self.env1_output, self.env2_output ],\n",
        "                           hidden_size=hidden_size\n",
        "                           )\n",
        "\n",
        "        self.log_std_env1 = nn.Parameter(torch.ones(1, self.env1_output) * std)\n",
        "        self.log_std_env2 = nn.Parameter(torch.ones(1, self.env2_output) * std)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.encoder.encode(self, x).to(device)\n",
        "        model_input = self.create_model_input(state, env_id)\n",
        "\n",
        "        value = self.critic(model_input)\n",
        "        mu    = self.actor(model_input)\n",
        "\n",
        "        if env_id == self.env1_id:   std = self.log_std_env1.exp().expand_as(mu)\n",
        "        elif env_id == self.env2_id: std = self.log_std_env2.exp().expand_as(mu)\n",
        "\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value\n",
        "\n",
        "    def create_model_input(self, state, env_id):\n",
        "         return {'state':state, 'env_id': env_id}\n",
        "\n",
        "    def save(self, name = 'model.pt', do_print=True ):\n",
        "        if do_print==True: print(\"saving weight model\")\n",
        "\n",
        "        torch.save(self.state_dict(),name )\n",
        "\n",
        "    def load(self, name = 'model.pt'):\n",
        "        self.load_state_dict(torch.load(name) )\n",
        "        print(f\"loaded: {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mku5EnhMtdBs"
      },
      "source": [
        "#Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "5pIDF5hftdBs"
      },
      "outputs": [],
      "source": [
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "3ehz4izJtdBt"
      },
      "outputs": [],
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
        "\n",
        "def ppo_update(model, hidden_cascade, optimizer, env_id, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "\n",
        "            state = np.array(state)\n",
        "            model_input = model.create_model_input(state, env_id)\n",
        "            dist, value = model(model_input) #model(state)\n",
        "\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)   # calculating log probability on actions\n",
        "\n",
        "            ratio = (new_log_probs - old_log_probs).exp()  # should not be the fraction instead difference?\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
        "\n",
        "            '''\n",
        "            ### new code ###\n",
        "            print(f\"original loss: {loss}\")\n",
        "            new_probs = new_log_probs.exp()\n",
        "\n",
        "            value_kl_penalty = kl_penalty( hidden_cascade=hidden_cascade,\n",
        "                                           actual_policy=new_probs,\n",
        "                                           old_log_policy=old_log_probs\n",
        "                                           )\n",
        "\n",
        "            value_loss_casc  = loss_casc(  hidden_cascade=hidden_cascade,\n",
        "                                           actual_policy=new_probs,\n",
        "                                           old_log_policy=old_log_probs\n",
        "                                           )\n",
        "\n",
        "            loss = loss + value_kl_penalty + value_loss_casc\n",
        "            print(f\"new loss: {loss}\\n\")\n",
        "            ### end new code ###\n",
        "            '''\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kl_penalty(hidden_cascade, actual_policy, old_log_policy, beta=0.5, omega=2.0, k=3):\n",
        "    kldiv_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    loss = 0\n",
        "\n",
        "    for _ in range(len(hidden_cascade.keys())):\n",
        "        loss = - beta*(omega**i) * kldiv_loss(actual_policy, old_log_policy )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "LVPng0Sks5H-"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_casc(hidden_cascade, actual_policy, old_log_policy, omega=2.0, omega12=0.25, k=3):\n",
        "    kldiv_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "\n",
        "    loss = - omega12*kldiv_loss( actual_policy[0], old_log_policy[1] )\n",
        "\n",
        "    for i in range(len(hidden_cascade.keys())):\n",
        "        if i==0: continue\n",
        "\n",
        "        loss += - kldiv_loss( actual_policy[i], old_log_policy[i-1] )*omega\n",
        "\n",
        "        if i < ( len(hidden_cascade) -1 ): # not doing updated on the last one\n",
        "            loss += - kldiv_loss( actual_policy[i], old_log_policy[i+1] )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "iIQUzEAUuH92"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZRZqea-qeB"
      },
      "source": [
        "#Main program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6oG4o52CyUV"
      },
      "source": [
        "##Initialization for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KLi-3vI-qeC",
        "outputId": "45b1b4fc-71e7-42f5-e84b-0804d7925efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_inputs1: 3, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 2, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n"
          ]
        }
      ],
      "source": [
        "num_envs = 16\n",
        "\n",
        "env1_id = \"Pendulum-v1\"\n",
        "env2_id = \"MountainCarContinuous-v0\"\n",
        "env_ids = [env1_id, env2_id ]\n",
        "\n",
        "env1 = gym.make(env1_id)\n",
        "env2 = gym.make(env2_id)\n",
        "single_envs = { env1_id: env1,\n",
        "                env2_id: env2\n",
        "               }\n",
        "\n",
        "num_inputs_1  = env1.observation_space.shape[0];    num_outputs_1 = env1.action_space.shape[0]\n",
        "num_inputs_2  = env2.observation_space.shape[0];    num_outputs_2 = env2.action_space.shape[0]\n",
        "\n",
        "print(f\"num_inputs1: {num_inputs_1}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "print(f\"num_inputs2: {num_inputs_2}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "envs1 = make_vec_env(env1_name, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_name, n_envs=num_envs)\n",
        "\n",
        "vectorized_envs = { env1_id: envs1,\n",
        "                    env2_id: envs2\n",
        "                    }\n",
        "\n",
        "#Hyper params:\n",
        "hidden_size      = 256\n",
        "lr               = 3e-4\n",
        "num_steps        = 20\n",
        "mini_batch_size  = 5\n",
        "ppo_epochs       = 4\n",
        "threshold_reward = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFAGCHGGA4tx",
        "outputId": "bc9c888f-0596-4a95-8f4e-08091f4754c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "dict_keys(['network_PC', 'network_2', 'network_3', 'network_4'])\n"
          ]
        }
      ],
      "source": [
        "n_hidden_network = 4\n",
        "hidden_network = {}\n",
        "for i in range(4):\n",
        "    hidden_id = 'network'\n",
        "    if i == 0: hidden_id += '_PC'\n",
        "    else: hidden_id += '_' + str(i+1)\n",
        "\n",
        "    hidden_network[hidden_id] = ActorCritic(envs=vectorized_envs, env_ids=env_ids, hidden_size=256, lr=lr).to(device)\n",
        "\n",
        "print(hidden_network.keys())\n",
        "\n",
        "#model_PC = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)\n",
        "#model_2 = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)\n",
        "#model_3 = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)\n",
        "#model_4 = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPBetpLoC3sa"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "BPsNLqs9-qeD"
      },
      "outputs": [],
      "source": [
        "def swtich_enviroment( vectorized_envs, env_ids, env_index):\n",
        "    env_index += 1\n",
        "    new_id_env = env_ids[ env_index % 2 ]   # -------- the switch is only for the first two environments\n",
        "    new_env = vectorized_envs[ new_id_env ]\n",
        "    return new_env, new_id_env, env_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "XUug4y3KCNMX"
      },
      "outputs": [],
      "source": [
        "def train(main_model, hidden_networks, main_env, optimizer, vect_envs, train_id_env, state, max_frames, early_stop, test_rewards, plot_frequency=1000, plot=True):\n",
        "    frame_idx = 0\n",
        "    while frame_idx < max_frames and not early_stop:\n",
        "        print(f\"\\rit: {frame_idx}/{max_frames}\", end=\"\")\n",
        "        log_probs = []\n",
        "        values    = []\n",
        "        states    = []\n",
        "        actions   = []\n",
        "        rewards   = []\n",
        "        masks     = []\n",
        "        entropy = 0\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "\n",
        "            model_input = main_model.create_model_input(state, train_id_env)\n",
        "            dist, value = main_model(model_input)\n",
        "\n",
        "            action = dist.sample()\n",
        "            next_state, reward, done, _ = vect_envs.step(action.cpu().numpy())\n",
        "\n",
        "            log_prob = dist.log_prob(action)\n",
        "            entropy += dist.entropy().mean()\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "            masks.append(torch.FloatTensor(1-done).unsqueeze(1).to(device))\n",
        "\n",
        "            states.append(torch.FloatTensor(model_input['state'] ).to(device) )\n",
        "            actions.append(action)\n",
        "\n",
        "            state = next_state\n",
        "            frame_idx += 1\n",
        "\n",
        "            if plot==True and frame_idx % plot_frequency == 0:\n",
        "                test_reward = np.mean([test_env(main_model, main_env, train_id_env) for _ in range(10)])\n",
        "                test_rewards.append(test_reward)\n",
        "                plot(train_id_env, frame_idx, test_rewards)\n",
        "                if test_reward > threshold_reward: early_stop = True\n",
        "\n",
        "\n",
        "        model_input = main_model.create_model_input(state, train_id_env)\n",
        "        _, next_value = main_model(model_input)\n",
        "\n",
        "        # try with simple TD-error\n",
        "        returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "        returns   = torch.cat(returns).detach()\n",
        "        log_probs = torch.cat(log_probs).detach()\n",
        "        values    = torch.cat(values).detach()\n",
        "        states    = torch.cat(states)\n",
        "        actions   = torch.cat(actions)\n",
        "        advantage = returns - values\n",
        "\n",
        "        ppo_update(main_model, hidden_networks, optimizer, train_id_env, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "t-LEJNaxtdBu",
        "outputId": "2eda86c8-72ae-4319-a233-ba247e5ed7f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded: model.pt\n",
            "saving weight model\n",
            "entered\n",
            "e: 0, actual_id_env: Pendulum-v1, env_index: 2, switch: 1\n",
            "it: 20/50000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-2b85c4fa4853>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mactual_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactual_id_env\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     train(main_model=model_PC,\n\u001b[0m\u001b[1;32m     44\u001b[0m           \u001b[0mhidden_networks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_network\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0mmain_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactual_single_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-744689a141aa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(main_model, hidden_networks, main_env, optimizer, vect_envs, train_id_env, state, max_frames, early_stop, test_rewards, plot_frequency, plot)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mppo_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_networks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_id_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppo_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-da30586f07a8>\u001b[0m in \u001b[0;36mppo_update\u001b[0;34m(model, hidden_cascade, optimizer, env_id, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param)\u001b[0m\n\u001b[1;32m     45\u001b[0m             '''\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epoch = 10\n",
        "epoch_per_train = 50000\n",
        "\n",
        "swtich_counter = 0\n",
        "switch_env_frequency = 1\n",
        "\n",
        "early_stop = False\n",
        "#test_rewards = []\n",
        "\n",
        "\n",
        "env_states = {}     # contain state to pass when callicng act\n",
        "env_step   = {}     # counter for switch\n",
        "env_reward = {}     # counter for reward\n",
        "env_index  = 1\n",
        "\n",
        "\n",
        "for id_env in vectorized_envs.keys():\n",
        "    env_states[id_env] = vectorized_envs[id_env].reset()\n",
        "    env_reward[id_env] = []\n",
        "    env_step[id_env]   = 0\n",
        "\n",
        "actual_id_env = env_ids[env_index]\n",
        "actual_vectorized_env = vectorized_envs[actual_id_env]\n",
        "actual_single_env = single_envs[actual_id_env]\n",
        "\n",
        "model_PC = hidden_network['network_PC']\n",
        "model_PC.load()\n",
        "optimizer = optim.Adam(model_PC.parameters(), lr=lr)\n",
        "\n",
        "for e in range(epoch):\n",
        "    model_PC.save()\n",
        "\n",
        "    #if e > 1 and ( e %  switch_env_frequency == 0):\n",
        "    if  e %  switch_env_frequency == 0:\n",
        "        print(\"entered\")\n",
        "        actual_vectorized_env, actual_id_env, env_index = swtich_enviroment(vectorized_envs, env_ids, env_index)\n",
        "        actual_single_env=single_envs[actual_id_env]\n",
        "        swtich_counter += 1\n",
        "\n",
        "    print(f\"e: {e}, actual_id_env: {actual_id_env}, env_index: {env_index}, switch: {swtich_counter}\")\n",
        "\n",
        "    actual_state = env_states[actual_id_env]\n",
        "    train(main_model=model_PC,\n",
        "          hidden_networks=hidden_network,\n",
        "          main_env=actual_single_env,\n",
        "          optimizer=optimizer,\n",
        "          vect_envs=actual_vectorized_env,\n",
        "          train_id_env=actual_id_env,\n",
        "          state=actual_state,\n",
        "          max_frames=epoch_per_train,\n",
        "          early_stop=early_stop,\n",
        "          test_rewards = env_reward[actual_id_env],\n",
        "          plot_frequency=10000,\n",
        "          plot=False\n",
        "          )\n",
        "#UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
        "# return torch.tensor(encoded_states, dtype=torch.float32)\n",
        "#solve this on encoder\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}