{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSHiaIsp_hxu"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "ccdKZ4CltkW3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "DmPSS_NatdBk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn.preprocessing\n",
        "from sklearn.kernel_approximation import RBFSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "Refo9Y_vtdBn"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "Eq5dx6SGtdBp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6ee7f7d0-367f-4099-ffe5-dc1c8d56e1eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnum_envs = 16\\n\\nenv1_name = \"Pendulum-v1\"\\nenv2_name = \"MountainCarContinuous-v0\"\\n\\nenv1 = gym.make(env1_name)\\nenv2 = gym.make(env2_name)\\n\\nenvs1 = make_vec_env(env1_name, n_envs=num_envs)\\nenvs2 = make_vec_env(env2_name, n_envs=num_envs)\\n\\np_envs_ids = [env1_name, env2_name ]\\np_envs = {env1_name: envs1,\\n          env2_name: envs2\\n          }\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "samples = [[-0.6810, -0.7323, -0.3519],\n",
        "        [ 0.9754, -0.2205, -0.9598],\n",
        "        [ 0.8309, -0.5564, -0.8232],\n",
        "        [ 0.4757,  0.8796,  0.8216],\n",
        "        [-0.9221, -0.3869,  0.8180],\n",
        "        [-0.9108, -0.4129,  0.2524],\n",
        "        [-0.1120, -0.9937, -0.8391],\n",
        "        [ 0.5760,  0.8175,  0.6597],\n",
        "        [ 0.4632, -0.8863, -0.1590],\n",
        "        [ 0.5347,  0.8450, -0.4712],\n",
        "        [ 0.5242,  0.8516, -0.9269],\n",
        "        [-0.8494, -0.5277,  0.9623],\n",
        "        [ 0.8243, -0.5661, -0.8888],\n",
        "        [-0.9457, -0.3252, -0.5394],\n",
        "        [ 0.9998, -0.0188, -0.7901],\n",
        "        [-0.9918, -0.1282,  0.9201]]\n",
        "\n",
        "'''\n",
        "num_envs = 16\n",
        "\n",
        "env1_name = \"Pendulum-v1\"\n",
        "env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "env1 = gym.make(env1_name)\n",
        "env2 = gym.make(env2_name)\n",
        "\n",
        "envs1 = make_vec_env(env1_name, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_name, n_envs=num_envs)\n",
        "\n",
        "p_envs_ids = [env1_name, env2_name ]\n",
        "p_envs = {env1_name: envs1,\n",
        "          env2_name: envs2\n",
        "          }\n",
        "'''\n",
        "#model_ac = ActorCritic(envs=p_envs, env_ids=p_envs_ids, hidden_size=256).to(device)\n",
        "#model_input = model_ac.create_model_input(samples, env1_name)\n",
        "\n",
        "#dist, value = model_ac(model_input)\n",
        "#print(f\"dist: {dist}, val: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy_Ly0wC_Yiv"
      },
      "source": [
        "#Auxiliar function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "zbZ12TwvtdBr"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "def plot(name, frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title(name + ': ' + 'frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "\n",
        "def plot_all(names, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(15,5))  # Adjust the size as needed\n",
        "\n",
        "    last_reward = {}\n",
        "    for i, name in enumerate(names):\n",
        "        plt.subplot(1, 2, i + 1)  # This creates subplots in a 1 row, 2 columns format\n",
        "\n",
        "        if len(rewards[name]) == 0: last_reward[name] = 'NaN'\n",
        "        else: last_reward[name] = rewards[name][-1]\n",
        "\n",
        "        plt.title(f'{name}, reward: {[last_reward[name]]}')\n",
        "        plt.plot(rewards[name])\n",
        "\n",
        "    plt.tight_layout()  # This automatically adjusts subplots to fit into the figure area.\n",
        "    plt.show()\n",
        "\n",
        "def test_env(model, env, id_env, max_step, vis=False):\n",
        "\n",
        "    state = torch.FloatTensor(env.reset()[0]).unsqueeze(0)\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "    while not done:\n",
        "        #state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        #print(state)\n",
        "        #state = torch.FloatTensor(state).to(device)\n",
        "        #dist, _ = model(model_input)\n",
        "        state = np.array(state)\n",
        "        model_input = model.create_model_input(state, id_env)\n",
        "        dist, _ = model(model_input)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(dist.sample().cpu().numpy())\n",
        "        done = terminated or truncated\n",
        "\n",
        "\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        #state = np.expand_dims( np.array(next_state.squeeze(-1)), 0 )\n",
        "        #print(f\"sq: {[next_state.squeeze(-1)]}\")\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "        if step >= max_step:\n",
        "            break\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "#p_env1_name = \"Pendulum-v1\"\n",
        "#p_env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "#p_env1 = gym.make(env1_name)\n",
        "#p_env2 = gym.make(env2_name)\n",
        "\n",
        "#model_PC.load()\n",
        "#print( test_env(model_PC, p_env1, p_env1_name ))\n",
        "#print( test_env(model_PC, p_env2, p_env2_name) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8L3VZyPtdBq"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "lcolBdfL-qd2"
      },
      "outputs": [],
      "source": [
        "class RBFFeatureEncoder:\n",
        "    def __init__(self, env1, env2, env3, n_component=100):\n",
        "\n",
        "        self.env1 = env1\n",
        "        self.env2 = env2\n",
        "        self.env3 = env3\n",
        "\n",
        "        data1 = np.array([env1.observation_space.sample() for x in range(10000)]); data1 = np.clip(data1, -1e38, 1e38)\n",
        "        data2 = np.array([env2.observation_space.sample() for x in range(10000)]); data2 = np.clip(data2, -1e38, 1e38)\n",
        "        data3 = np.array([env3.observation_space.sample() for x in range(10000)]); data3 = np.clip(data3, -1e38, 1e38)\n",
        "\n",
        "        self.rbf_sampler1 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler2 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler3 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "\n",
        "        self.standard_scaler1 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler2 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler3 = sklearn.preprocessing.StandardScaler()\n",
        "\n",
        "        self.standard_scaler1.fit(data1)\n",
        "        self.standard_scaler2.fit(data2)\n",
        "        self.standard_scaler3.fit(data3)\n",
        "\n",
        "        transformed_data1 = self.standard_scaler1.transform(data1)\n",
        "        transformed_data2 = self.standard_scaler2.transform(data2)\n",
        "        transformed_data3 = self.standard_scaler3.transform(data3)\n",
        "\n",
        "        self.rbf_sampler1.fit( transformed_data1 )\n",
        "        self.rbf_sampler2.fit( transformed_data2 )\n",
        "        self.rbf_sampler3.fit( transformed_data3 )\n",
        "\n",
        "    def wrapper_encode(self, model, model_input):\n",
        "\n",
        "        state = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "\n",
        "        if model.env1_id == env_id:\n",
        "            transformed_state = self.standard_scaler1.transform([state])\n",
        "            encoded_state = self.rbf_sampler1.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env2_id == env_id:\n",
        "            transformed_state = self.standard_scaler2.transform([state])\n",
        "            encoded_state = self.rbf_sampler2.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env3_id == env_id:\n",
        "            transformed_state = self.standard_scaler3.transform([state])\n",
        "            encoded_state = self.rbf_sampler3.transform(transformed_state).flatten()\n",
        "\n",
        "        return encoded_state\n",
        "\n",
        "    def encode(self, model, model_input):\n",
        "\n",
        "        states = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "        encoded_states = []\n",
        "\n",
        "        for s in states:\n",
        "            new_input = model.create_model_input(s,env_id )\n",
        "            encoded_states.append( self.wrapper_encode(model, new_input) )\n",
        "\n",
        "        return torch.tensor(encoded_states, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self.rbf_sampler1.n_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "1-VdeX7z-qd4"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "        state = self.output_layer(state)\n",
        "        return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "ZZfIK7Nu-qd5"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_size, env_outputs, hidden_size, env_ids):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "        self.env1_actions = env_outputs[0]\n",
        "        self.env2_actions = env_outputs[1]\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        self.env1_output = nn.Linear(hidden_size, self.env1_actions)\n",
        "        self.env2_output = nn.Linear(hidden_size, self.env2_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "\n",
        "        if env_id == self.env1_id:   mu = self.env1_output(state)\n",
        "        elif env_id == self.env2_id: mu = self.env2_output(state)\n",
        "\n",
        "        return mu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "llFUdL-x-qd6"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, envs, env_ids, hidden_size, std=0.0, lr=0.0001):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "\n",
        "        self.envs1   = envs[self.env1_id]\n",
        "        self.envs2   = envs[self.env2_id]\n",
        "\n",
        "        self.env1_output = self.envs1.action_space.shape[0]\n",
        "        self.env2_output = self.envs2.action_space.shape[0]\n",
        "\n",
        "        self.encoder    = RBFFeatureEncoder(self.envs1, self.envs2, self.envs2)\n",
        "        self.input_size = self.encoder.size\n",
        "\n",
        "        print(f\"num_inputs1: {self.input_size}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "        print(f\"num_inputs2: {self.input_size}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "\n",
        "        self.critic = Critic(input_size=self.input_size,\n",
        "                             hidden_size=hidden_size\n",
        "                             )\n",
        "\n",
        "        self.actor = Actor(input_size=self.input_size,\n",
        "                           env_ids=env_ids,\n",
        "                           env_outputs=[self.env1_output, self.env2_output ],\n",
        "                           hidden_size=hidden_size\n",
        "                           )\n",
        "\n",
        "        self.log_std_env1 = nn.Parameter(torch.ones(1, self.env1_output) * std)\n",
        "        self.log_std_env2 = nn.Parameter(torch.ones(1, self.env2_output) * std)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.encoder.encode(self, x).to(device)\n",
        "        model_input = self.create_model_input(state, env_id)\n",
        "\n",
        "        value = self.critic(model_input)\n",
        "        mu    = self.actor(model_input)\n",
        "\n",
        "        if env_id == self.env1_id:   std = self.log_std_env1.exp().expand_as(mu)\n",
        "        elif env_id == self.env2_id: std = self.log_std_env2.exp().expand_as(mu)\n",
        "\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value\n",
        "\n",
        "    def create_model_input(self, state, env_id):\n",
        "         return {'state':state, 'env_id': env_id}\n",
        "\n",
        "    def save(self, name = 'model.pt', do_print=True ):\n",
        "        if do_print==True: print(\"saving weight model\")\n",
        "\n",
        "        torch.save(self.state_dict(),name )\n",
        "\n",
        "    def load(self, name = 'model.pt'):\n",
        "        self.load_state_dict(torch.load(name) )\n",
        "        print(f\"loaded: {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mku5EnhMtdBs"
      },
      "source": [
        "#Loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kldiv_loss = nn.KLDivLoss(reduction=\"batchmean\")"
      ],
      "metadata": {
        "id": "OPtzy_h2tNt4"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "5pIDF5hftdBs"
      },
      "outputs": [],
      "source": [
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ppo_iter(hidden_network, mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states['network_PC'].size(0)\n",
        "\n",
        "    mini_batch_states = {net: [] for net in states.keys()}\n",
        "    mini_batch_actions = {net: [] for net in actions.keys()}\n",
        "    mini_batch_log_probs = {net: [] for net in log_probs.keys()}\n",
        "    mini_batch_returns = {net: [] for net in returns.keys()}\n",
        "    mini_batch_advantage = {net: [] for net in advantage.keys()}\n",
        "\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "\n",
        "        for net in hidden_network.keys():\n",
        "            mini_batch_states[net].append(states[net][rand_ids, :])\n",
        "            mini_batch_actions[net].append(actions[net][rand_ids, :])\n",
        "            mini_batch_log_probs[net].append(log_probs[net][rand_ids, :])\n",
        "            mini_batch_returns[net].append(returns[net][rand_ids, :])\n",
        "            mini_batch_advantage[net].append(advantage[net][rand_ids, :])\n",
        "\n",
        "    ret_dict = {'states': mini_batch_states,\n",
        "                'actions': mini_batch_actions,\n",
        "                'log_probs':mini_batch_log_probs,\n",
        "                'returns': mini_batch_returns,\n",
        "                'advantage': mini_batch_advantage\n",
        "                }\n",
        "\n",
        "    return ret_dict"
      ],
      "metadata": {
        "id": "_yXIt8VRwMNG"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE6mw7bitQVe",
        "outputId": "26c0cf97-a5d4-4346-875b-33b6ac443859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def ppo_update(hidden_network, env_id, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2, beta=0.5, omega=4, omega12=1):\n",
        "\n",
        "    for _ in range(ppo_epochs):\n",
        "        mini_batches = ppo_iter(hidden_network, mini_batch_size, states, actions, log_probs, returns, advantages)\n",
        "        print(f\"len ret: {len(mini_batches)}, type: {type(mini_batches['states'])}\")\n",
        "        print(mini_batches.keys())\n",
        "\n",
        "        batch_size = states['network_PC'].size(0)\n",
        "        for i in range(batch_size // mini_batch_size):\n",
        "\n",
        "            new_log_probs = {}\n",
        "            ppo_loss = 0\n",
        "            casc_loss = 0\n",
        "\n",
        "            for NET in hidden_network.keys():\n",
        "\n",
        "                states = mini_batches['states'][NET][i]\n",
        "                actions = mini_batches['actions'][NET][i]\n",
        "                old_log_probs = mini_batches['log_probs'][NET][i]\n",
        "                returns = mini_batches['returns'][NET][i]\n",
        "                advantage = mini_batches['advantage'][NET][i]\n",
        "\n",
        "                state = np.array(mini_batches['states']['network_PC'][i])\n",
        "\n",
        "                model_input[state] = hidden_network[NET].create_model_input(state, env_id)\n",
        "\n",
        "                action[NET] = dist[NET].sample()\n",
        "                next_state[NET], reward[NET], done[NET], _ = actual_vectorized_env.step(action[NET].cpu().numpy())\n",
        "\n",
        "                new_log_probs[NET] = dist[NET].log_prob(action[NET])\n",
        "                entropy[NET] += dist[NET].entropy().mean()\n",
        "\n",
        "                ppo_loss += -beta*(omega**(i))*kldiv_loss(old_log_probs[NET], new_log_probs[NET].exp())\n",
        "\n",
        "                if i == 0:\n",
        "                    casc_loss += -(omega12*kldiv_loss(old_log_probs[net_id + '_' + str(i+2)], new_log_probs[NET].exp()))\n",
        "                elif i == cascade_nets-1:\n",
        "                    casc_loss += -(omega*kldiv_loss(old_log_probs[net_id + '_' + str(i)], new_log_probs[NET].exp())+kldiv_loss(new_log_probs[NET], new_log_probs[NET].exp()) )\n",
        "                else:\n",
        "                    casc_loss += -(omega*kldiv_loss(old_log_probs[net_id + '_' + str(i)], new_log_probs[NET].exp())+kldiv_loss(old_log_probs[net_id + '_' + str(i+2)], new_log_probs[NET].exp()) )\n",
        "\n",
        "\n",
        "            ratio = (new_log_probs['network_PC'] - old_log_probs['network_PC']).exp()\n",
        "            surr1 = ratio * advantage['network_PC']\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage['network_PC']\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (returns - value).pow(2).mean()\n",
        "\n",
        "            pg_loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy['network_PC']\n",
        "\n",
        "            total_loss = ppo_loss.mean() + casc_loss.mean() + pg_loss\n",
        "\n",
        "            for net in hidden_network.keys():\n",
        "                hidden_network[net].optimizer.zero_grad()\n",
        "\n",
        "            total_loss.backward()\n",
        "\n",
        "            for net in hidden_network.keys():\n",
        "                hidden_network[net].optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZRZqea-qeB"
      },
      "source": [
        "#Main program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6oG4o52CyUV"
      },
      "source": [
        "##Initialization for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "BPsNLqs9-qeD"
      },
      "outputs": [],
      "source": [
        "def swtich_enviroment( vectorized_envs, env_ids, env_index):\n",
        "    env_index += 1\n",
        "    new_id_env = env_ids[ env_index % 2 ]   # -------- the switch is only for the first two environments\n",
        "    new_env = vectorized_envs[ new_id_env ]\n",
        "    return new_env, new_id_env, env_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KLi-3vI-qeC",
        "outputId": "bd3db803-5017-4279-927d-59f8f646f6c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_inputs1: 3, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 2, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n"
          ]
        }
      ],
      "source": [
        "num_envs = 16\n",
        "\n",
        "env1_id = \"Pendulum-v1\"\n",
        "env2_id = \"MountainCarContinuous-v0\"\n",
        "env_ids = [env1_id, env2_id ]\n",
        "\n",
        "env1 = gym.make(env1_id)\n",
        "env2 = gym.make(env2_id)\n",
        "single_envs = { env1_id: env1,\n",
        "                env2_id: env2\n",
        "               }\n",
        "\n",
        "num_inputs_1  = env1.observation_space.shape[0];    num_outputs_1 = env1.action_space.shape[0]\n",
        "num_inputs_2  = env2.observation_space.shape[0];    num_outputs_2 = env2.action_space.shape[0]\n",
        "\n",
        "print(f\"num_inputs1: {num_inputs_1}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "print(f\"num_inputs2: {num_inputs_2}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "envs1 = make_vec_env(env1_id, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_id, n_envs=num_envs)\n",
        "\n",
        "vectorized_envs = { env1_id: envs1,\n",
        "                    env2_id: envs2\n",
        "                    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Settings an dHyperparameters**"
      ],
      "metadata": {
        "id": "7u6gHkN0qTzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFAGCHGGA4tx",
        "outputId": "4c619396-f5db-410d-e005-a8519a14aa64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "dict_keys(['network_PC', 'network_PC_2', 'network_PC_2_3', 'network_PC_2_3_4'])\n"
          ]
        }
      ],
      "source": [
        "#Hyper params:\n",
        "hidden_size      = 256\n",
        "lr               = 3e-4\n",
        "num_steps        = 20\n",
        "mini_batch_size  = 5\n",
        "ppo_epochs       = 4\n",
        "env_thresholds = {env1_id:-200, env2_id:50}\n",
        "\n",
        "early_stop = True\n",
        "\n",
        "epoch = 18\n",
        "train_per_epoch = 30000\n",
        "\n",
        "swtich_counter = 0\n",
        "switch_env_frequency = 3\n",
        "\n",
        "cascade_nets = 4\n",
        "hidden_network = {}\n",
        "hidden_id = 'network'\n",
        "\n",
        "for i in range(cascade_nets):\n",
        "    if i == 0: hidden_id += '_PC'\n",
        "    else: hidden_id += '_' + str(i+1)\n",
        "    hidden_network[hidden_id] = ActorCritic(envs=vectorized_envs, env_ids=env_ids, hidden_size=256, lr=lr).to(device)\n",
        "\n",
        "print(hidden_network.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPBetpLoC3sa"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "t-LEJNaxtdBu",
        "outputId": "43b41227-f2c7-49c5-cc4c-2c6f9acb110d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving weight model\n",
            "e: 0, actual_id_env: Pendulum-v1, env_index: 0, switch: 0\n",
            "\rit: 0/30000\n",
            "len ret: 5, type: <class 'dict'>\n",
            "dict_keys(['states', 'actions', 'log_probs', 'returns', 'advantage'])\n",
            "dict_keys(['network_PC', 'network_PC_2', 'network_PC_2_3', 'network_PC_2_3_4'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'numpy.ndarray'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-295-4024cfb18a42>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m           \u001b[0madvantage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mppo_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_id_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppo_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-294-4e07af26326f>\u001b[0m in \u001b[0;36mppo_update\u001b[0;34m(hidden_network, env_id, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param, beta, omega, omega12)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'states'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'network_PC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mmodel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_network\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNET\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ],
      "source": [
        "env_states = {}     # contain state to pass when callicng act\n",
        "env_step   = {}     # counter for switch\n",
        "env_reward = {}     # counter for reward\n",
        "env_index  = 0\n",
        "\n",
        "for id_env in vectorized_envs.keys():\n",
        "    env_states[id_env] = vectorized_envs[id_env].reset()\n",
        "    env_reward[id_env] = []\n",
        "    env_step[id_env]   = 0\n",
        "\n",
        "actual_id_env = env_ids[env_index]\n",
        "actual_vectorized_env = vectorized_envs[actual_id_env]\n",
        "actual_single_env = single_envs[actual_id_env]\n",
        "\n",
        "for e in range(epoch):\n",
        "    hidden_network['network_PC'].save()\n",
        "\n",
        "    if e > 1 and ( e %  switch_env_frequency == 0):\n",
        "        actual_vectorized_env, actual_id_env, env_index = swtich_enviroment(vectorized_envs, env_ids, env_index)\n",
        "        actual_single_env=single_envs[actual_id_env]\n",
        "        swtich_counter += 1\n",
        "\n",
        "    print(f\"e: {e}, actual_id_env: {actual_id_env}, env_index: {env_index}, switch: {swtich_counter}\")\n",
        "\n",
        "    actual_state = env_states[actual_id_env]\n",
        "    frame_idx = 0\n",
        "    while frame_idx < train_per_epoch:\n",
        "        #print(f\"\\rit: {frame_idx}/{train_per_epoch}, loss: {loss}\", end=\"\")\n",
        "        print(f\"\\rit: {frame_idx}/{train_per_epoch}\")\n",
        "\n",
        "        log_probs   = {}; values     = {}; states    = {}\n",
        "        actions     = {}; rewards    = {}; masks     = {}\n",
        "        model_input = {}; dist       = {}; value     = {}\n",
        "        action      = {}; next_state = {}; reward    = {}\n",
        "        done        = {}; log_prob   = {}; entropy   = {}\n",
        "        next_value  = {}; returns    = {}; advantage = {}\n",
        "\n",
        "        for net in hidden_network.keys():\n",
        "            entropy[net]   = 0;  log_probs[net] = []; values[net]    = []\n",
        "            states[net]    = []; actions[net]   = []; rewards[net]   = []\n",
        "            masks[net]     = []\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "\n",
        "            for net in hidden_network.keys():\n",
        "              state = env_states[actual_id_env]\n",
        "              model_input[net] = hidden_network[net].create_model_input(state, actual_id_env)\n",
        "              dist[net], value[net] = hidden_network[net](model_input[net])\n",
        "\n",
        "              action[net] = dist[net].sample()\n",
        "              next_state[net], reward[net], done[net], _ = actual_vectorized_env.step(action[net].cpu().numpy())\n",
        "\n",
        "              log_prob[net] = dist[net].log_prob(action[net])\n",
        "              entropy[net] += dist[net].entropy().mean()\n",
        "\n",
        "              log_probs[net].append(log_prob[net])\n",
        "              values[net].append(value[net])\n",
        "              rewards[net].append(torch.FloatTensor(reward[net]).unsqueeze(1).to(device))\n",
        "              masks[net].append(torch.FloatTensor(1-done[net]).unsqueeze(1).to(device))\n",
        "\n",
        "              states[net].append(torch.FloatTensor(model_input[net]['state'] ).to(device) )\n",
        "              actions[net].append(action[net])\n",
        "\n",
        "            env_states[actual_id_env] = next_state['network_PC']\n",
        "            frame_idx += 1\n",
        "\n",
        "            if frame_idx % 1000 == 0:\n",
        "                test_rewards = env_reward[actual_id_env]\n",
        "                threshold    = env_thresholds[actual_id_env]\n",
        "\n",
        "                test_reward = np.mean([test_env(hidden_network['network_PC'], actual_single_env, actual_id_env) for _ in range(10)])\n",
        "                test_rewards.append(test_reward)\n",
        "\n",
        "                plot_all(env_ids, env_reward)\n",
        "                hidden_network['network_PC'].save(do_print=False)\n",
        "                if test_reward > threshold: early_stop = True\n",
        "\n",
        "        for net in hidden_network.keys():\n",
        "          model_input[net] = hidden_network[net].create_model_input(state, actual_id_env)\n",
        "          _, next_value[net] = hidden_network[net](model_input[net])\n",
        "\n",
        "          returns[net] = compute_gae(next_value[net], rewards[net], masks[net], values[net])\n",
        "\n",
        "          returns[net]   = torch.cat(returns[net]).detach()\n",
        "          log_probs[net] = torch.cat(log_probs[net]).detach()\n",
        "          values[net]    = torch.cat(values[net]).detach()\n",
        "          states[net]    = torch.cat(states[net])\n",
        "          actions[net]   = torch.cat(actions[net])\n",
        "          advantage[net] = returns[net] - values[net]\n",
        "\n",
        "        ppo_update(hidden_network, actual_id_env, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUug4y3KCNMX"
      },
      "outputs": [],
      "source": [
        "def train(main_model, hidden_networks, main_env, vect_envs, train_id_env, state, max_frames, early_stop, test_rewards, threshold_reward=-200, plot_frequency=1000, do_plot=True, all_ids=None, all_rewards=None):\n",
        "    frame_idx = 0\n",
        "    loss = 0\n",
        "    while frame_idx < max_frames and not early_stop:\n",
        "        print(f\"\\rit: {frame_idx}/{max_frames}, loss: {loss}\", end=\"\")\n",
        "        log_probs   = {}; values     = {}; states    = {}\n",
        "        actions     = {}; rewards    = {}; masks     = {}\n",
        "        model_input = {}; dist       = {}; value     = {}\n",
        "        action      = {}; next_state = {}; reward    = {}\n",
        "        done        = {}; log_prob   = {}; entropy   = {}\n",
        "        next_value  = {}; returns    = {}; advantage = {}\n",
        "\n",
        "        for net in hidden_network.keys():\n",
        "            entropy[net]   = 0;  log_probs[net] = []; values[net]    = []\n",
        "            states[net]    = []; actions[net]   = []; rewards[net]   = []\n",
        "            masks[net]     = []\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "\n",
        "            model_input = main_model.create_model_input(state, train_id_env)\n",
        "            dist, value = main_model(model_input)\n",
        "\n",
        "            action = dist.sample()\n",
        "            next_state, reward, done, _ = vect_envs.step(action.cpu().numpy())\n",
        "\n",
        "            log_prob = dist.log_prob(action)\n",
        "            entropy += dist.entropy().mean()\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "            masks.append(torch.FloatTensor(1-done).unsqueeze(1).to(device))\n",
        "\n",
        "            states.append(torch.FloatTensor(model_input['state'] ).to(device) )\n",
        "            actions.append(action)\n",
        "\n",
        "            state = next_state\n",
        "            frame_idx += 1\n",
        "\n",
        "            if do_plot==True and (frame_idx % plot_frequency == 0):\n",
        "                test_reward = np.mean([test_env(main_model, main_env, train_id_env, max_step=250) for _ in range(10)])\n",
        "                test_rewards.append(test_reward)\n",
        "                #plot(train_id_env, frame_idx, test_rewards)\n",
        "                plot_all(all_ids, all_rewards)\n",
        "                main_model.save(do_print=False)\n",
        "                if test_reward > threshold_reward[train_id_env]: early_stop = True\n",
        "\n",
        "\n",
        "        model_input = main_model.create_model_input(state, train_id_env)\n",
        "        _, next_value = main_model(model_input)\n",
        "\n",
        "        # try with simple TD-error\n",
        "        returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "        returns   = torch.cat(returns).detach()\n",
        "        log_probs = torch.cat(log_probs).detach()\n",
        "        values    = torch.cat(values).detach()\n",
        "        states    = torch.cat(states)\n",
        "        actions   = torch.cat(actions)\n",
        "        advantage = returns - values\n",
        "\n",
        "        loss = ppo_update(main_model, hidden_networks, train_id_env, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
        "        loss = torch.mean( torch.stack(loss).float() )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "train(main_model=model_PC,\n",
        "          hidden_networks=hidden_network,\n",
        "          main_env=actual_single_env,\n",
        "          vect_envs=actual_vectorized_env,\n",
        "          train_id_env=actual_id_env,\n",
        "          state=actual_state,\n",
        "          max_frames=epoch_per_train,\n",
        "          early_stop=early_stop,\n",
        "          test_rewards=env_reward[actual_id_env],\n",
        "          threshold_reward=env_thresholds,\n",
        "          plot_frequency=1000,\n",
        "          do_plot=True,\n",
        "          all_ids=env_ids,\n",
        "          all_rewards=env_reward\n",
        "          )\n",
        "'''\n",
        "#UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
        "# return torch.tensor(encoded_states, dtype=torch.float32)\n",
        "#solve this on encoder"
      ],
      "metadata": {
        "id": "HDWzRCl5pv54"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}