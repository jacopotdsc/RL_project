{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSHiaIsp_hxu"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "id": "ccdKZ4CltkW3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "id": "DmPSS_NatdBk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn.preprocessing\n",
        "from sklearn.kernel_approximation import RBFSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {
        "id": "Refo9Y_vtdBn"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "id": "Eq5dx6SGtdBp"
      },
      "outputs": [],
      "source": [
        "samples = [[-0.6810, -0.7323, -0.3519],\n",
        "        [ 0.9754, -0.2205, -0.9598],\n",
        "        [ 0.8309, -0.5564, -0.8232],\n",
        "        [ 0.4757,  0.8796,  0.8216],\n",
        "        [-0.9221, -0.3869,  0.8180],\n",
        "        [-0.9108, -0.4129,  0.2524],\n",
        "        [-0.1120, -0.9937, -0.8391],\n",
        "        [ 0.5760,  0.8175,  0.6597],\n",
        "        [ 0.4632, -0.8863, -0.1590],\n",
        "        [ 0.5347,  0.8450, -0.4712],\n",
        "        [ 0.5242,  0.8516, -0.9269],\n",
        "        [-0.8494, -0.5277,  0.9623],\n",
        "        [ 0.8243, -0.5661, -0.8888],\n",
        "        [-0.9457, -0.3252, -0.5394],\n",
        "        [ 0.9998, -0.0188, -0.7901],\n",
        "        [-0.9918, -0.1282,  0.9201]]\n",
        "\n",
        "num_envs = 16\n",
        "\n",
        "env1_name = \"Pendulum-v1\"\n",
        "env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "env1 = gym.make(env1_name)\n",
        "env2 = gym.make(env2_name)\n",
        "\n",
        "envs1 = make_vec_env(env1_name, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_name, n_envs=num_envs)\n",
        "\n",
        "p_envs_ids = [env1_name, env2_name ]\n",
        "p_envs = {env1_name: envs1,\n",
        "          env2_name: envs2\n",
        "          }\n",
        "\n",
        "#model_ac = ActorCritic(envs=p_envs, env_ids=p_envs_ids, hidden_size=256).to(device)\n",
        "#model_input = model_ac.create_model_input(samples, env1_name)\n",
        "\n",
        "#dist, value = model_ac(model_input)\n",
        "#print(f\"dist: {dist}, val: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy_Ly0wC_Yiv"
      },
      "source": [
        "#Auxiliar function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {
        "id": "zbZ12TwvtdBr"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)\n",
        "\n",
        "def plot(name, frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title(name + ': ' + 'frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "\n",
        "def plot_all(names, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(15,5))  # Adjust the size as needed\n",
        "\n",
        "    last_reward = {}\n",
        "    for i, name in enumerate(names):\n",
        "        plt.subplot(1, 2, i + 1)  # This creates subplots in a 1 row, 2 columns format\n",
        "\n",
        "        if len(rewards[name]) == 0: last_reward[name] = 'NaN'\n",
        "        else: last_reward[name] = rewards[name][-1]\n",
        "\n",
        "        plt.title(f'{name}, reward: {[last_reward[name]]}')\n",
        "        plt.plot(rewards[name])\n",
        "\n",
        "    plt.tight_layout()  # This automatically adjusts subplots to fit into the figure area.\n",
        "    plt.show()\n",
        "\n",
        "def test_env(model, env, id_env, max_step, vis=False):\n",
        "\n",
        "    state = torch.FloatTensor(env.reset()[0]).unsqueeze(0)\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "    while not done:\n",
        "        #state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        #print(state)\n",
        "        #state = torch.FloatTensor(state).to(device)\n",
        "        #dist, _ = model(model_input)\n",
        "        state = np.array(state)\n",
        "        model_input = model.create_model_input(state, id_env)\n",
        "        dist, _ = model(model_input)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(dist.sample().cpu().numpy())\n",
        "        done = terminated or truncated\n",
        "\n",
        "\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        #state = np.expand_dims( np.array(next_state.squeeze(-1)), 0 )\n",
        "        #print(f\"sq: {[next_state.squeeze(-1)]}\")\n",
        "        #state = torch.FloatTensor([next_state.squeeze(-1)])#.to(device)\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "        if step >= max_step:\n",
        "            break\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "#p_env1_name = \"Pendulum-v1\"\n",
        "#p_env2_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "#p_env1 = gym.make(env1_name)\n",
        "#p_env2 = gym.make(env2_name)\n",
        "\n",
        "#model_PC.load()\n",
        "#print( test_env(model_PC, p_env1, p_env1_name ) )\n",
        "#print( test_env(model_PC, p_env2, p_env2_name) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8L3VZyPtdBq"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "id": "lcolBdfL-qd2"
      },
      "outputs": [],
      "source": [
        "class RBFFeatureEncoder:\n",
        "    def __init__(self, env1, env2, env3, n_component=100):\n",
        "\n",
        "        self.env1 = env1\n",
        "        self.env2 = env2\n",
        "        self.env3 = env3\n",
        "\n",
        "        data1 = np.array([env1.observation_space.sample() for x in range(10000)]); data1 = np.clip(data1, -1e38, 1e38)\n",
        "        data2 = np.array([env2.observation_space.sample() for x in range(10000)]); data2 = np.clip(data2, -1e38, 1e38)\n",
        "        data3 = np.array([env3.observation_space.sample() for x in range(10000)]); data3 = np.clip(data3, -1e38, 1e38)\n",
        "\n",
        "        self.rbf_sampler1 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler2 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "        self.rbf_sampler3 = RBFSampler(gamma=0.999, n_components=n_component)\n",
        "\n",
        "        self.standard_scaler1 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler2 = sklearn.preprocessing.StandardScaler()\n",
        "        self.standard_scaler3 = sklearn.preprocessing.StandardScaler()\n",
        "\n",
        "        self.standard_scaler1.fit(data1)\n",
        "        self.standard_scaler2.fit(data2)\n",
        "        self.standard_scaler3.fit(data3)\n",
        "\n",
        "        transformed_data1 = self.standard_scaler1.transform(data1)\n",
        "        transformed_data2 = self.standard_scaler2.transform(data2)\n",
        "        transformed_data3 = self.standard_scaler3.transform(data3)\n",
        "\n",
        "        self.rbf_sampler1.fit( transformed_data1 )\n",
        "        self.rbf_sampler2.fit( transformed_data2 )\n",
        "        self.rbf_sampler3.fit( transformed_data3 )\n",
        "\n",
        "    def wrapper_encode(self, model, model_input):\n",
        "\n",
        "        state = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "\n",
        "        if model.env1_id == env_id:\n",
        "            transformed_state = self.standard_scaler1.transform([state])\n",
        "            encoded_state = self.rbf_sampler1.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env2_id == env_id:\n",
        "            transformed_state = self.standard_scaler2.transform([state])\n",
        "            encoded_state = self.rbf_sampler2.transform(transformed_state).flatten()\n",
        "\n",
        "        elif model.env3_id == env_id:\n",
        "            transformed_state = self.standard_scaler3.transform([state])\n",
        "            encoded_state = self.rbf_sampler3.transform(transformed_state).flatten()\n",
        "\n",
        "        return encoded_state\n",
        "\n",
        "    def encode(self, model, model_input):\n",
        "\n",
        "        states = model_input['state']\n",
        "        env_id = model_input['env_id']\n",
        "        encoded_states = []\n",
        "\n",
        "        for s in states:\n",
        "            new_input = model.create_model_input(s,env_id )\n",
        "            encoded_states.append( self.wrapper_encode(model, new_input) )\n",
        "\n",
        "        return torch.tensor(encoded_states, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self.rbf_sampler1.n_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {
        "id": "1-VdeX7z-qd4"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "        state = self.output_layer(state)\n",
        "        return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {
        "id": "ZZfIK7Nu-qd5"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_size, env_outputs, hidden_size, env_ids):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "        self.env1_actions = env_outputs[0]\n",
        "        self.env2_actions = env_outputs[1]\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        self.env1_output = nn.Linear(hidden_size, self.env1_actions)\n",
        "        self.env2_output = nn.Linear(hidden_size, self.env2_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.input_layer(state)\n",
        "        state = self.relu(state)\n",
        "\n",
        "        if env_id == self.env1_id:   mu = self.env1_output(state)\n",
        "        elif env_id == self.env2_id: mu = self.env2_output(state)\n",
        "\n",
        "        return mu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {
        "id": "llFUdL-x-qd6"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, envs, env_ids, hidden_size, std=0.0, lr=0.0001):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.env1_id = env_ids[0]\n",
        "        self.env2_id = env_ids[1]\n",
        "\n",
        "        self.envs1   = envs[self.env1_id]\n",
        "        self.envs2   = envs[self.env2_id]\n",
        "\n",
        "        self.env1_output = self.envs1.action_space.shape[0]\n",
        "        self.env2_output = self.envs2.action_space.shape[0]\n",
        "\n",
        "        self.encoder    = RBFFeatureEncoder(self.envs1, self.envs2, self.envs2)\n",
        "        self.input_size = self.encoder.size\n",
        "\n",
        "        print(f\"num_inputs1: {self.input_size}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "        print(f\"num_inputs2: {self.input_size}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "\n",
        "        self.critic = Critic(input_size=self.input_size,\n",
        "                             hidden_size=hidden_size\n",
        "                             )\n",
        "\n",
        "        self.actor = Actor(input_size=self.input_size,\n",
        "                           env_ids=env_ids,\n",
        "                           env_outputs=[self.env1_output, self.env2_output ],\n",
        "                           hidden_size=hidden_size\n",
        "                           )\n",
        "\n",
        "        self.log_std_env1 = nn.Parameter(torch.ones(1, self.env1_output) * std)\n",
        "        self.log_std_env2 = nn.Parameter(torch.ones(1, self.env2_output) * std)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        state  = x['state']\n",
        "        env_id = x['env_id']\n",
        "\n",
        "        state = self.encoder.encode(self, x).to(device)\n",
        "        model_input = self.create_model_input(state, env_id)\n",
        "\n",
        "        value = self.critic(model_input)\n",
        "        mu    = self.actor(model_input)\n",
        "\n",
        "        if env_id == self.env1_id:   std = self.log_std_env1.exp().expand_as(mu)\n",
        "        elif env_id == self.env2_id: std = self.log_std_env2.exp().expand_as(mu)\n",
        "\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value\n",
        "\n",
        "    def create_model_input(self, state, env_id):\n",
        "         return {'state':state, 'env_id': env_id}\n",
        "\n",
        "    def save(self, name = 'model.pt', do_print=True ):\n",
        "        if do_print==True: print(\"saving weight model\")\n",
        "\n",
        "        torch.save(self.state_dict(),name )\n",
        "\n",
        "    def load(self, name = 'model.pt'):\n",
        "        self.load_state_dict(torch.load(name) )\n",
        "        print(f\"loaded: {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mku5EnhMtdBs"
      },
      "source": [
        "#Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {
        "id": "5pIDF5hftdBs"
      },
      "outputs": [],
      "source": [
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "id": "3ehz4izJtdBt"
      },
      "outputs": [],
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
        "\n",
        "def ppo_update(model, hidden_cascade, optimizer, env_id, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    total_loss = []\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "\n",
        "            state = np.array(state)\n",
        "            model_input = model.create_model_input(state, env_id)\n",
        "            dist, value = model(model_input) #model(state)\n",
        "\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)   # calculating log probability on actions\n",
        "\n",
        "            ratio = (new_log_probs - old_log_probs).exp()  # should not be the fraction instead difference?\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
        "\n",
        "\n",
        "            ### new code ###\n",
        "            #print(f\"original loss: {loss}\")\n",
        "            new_probs = new_log_probs.exp()\n",
        "\n",
        "            value_kl_penalty1 = kl_penalty( hidden_cascade=hidden_cascade,\n",
        "                                           actual_policy=new_probs,\n",
        "                                           old_log_policy=old_log_probs\n",
        "                                           )\n",
        "            value_kl_penalty2 = torch.clamp(value_kl_penalty1, 1.0 - clip_param, 1.0 + clip_param)\n",
        "            value_kl_penalty  = torch.min(value_kl_penalty1, value_kl_penalty2)\n",
        "\n",
        "\n",
        "            value_loss_casc1  = loss_casc(  hidden_cascade=hidden_cascade,\n",
        "                                           actual_policy=new_probs,\n",
        "                                           old_log_policy=old_log_probs\n",
        "                                           )\n",
        "            value_loss_casc2 = torch.clamp(value_loss_casc1, 1.0 - clip_param, 1.0 + clip_param)\n",
        "            value_loss_casc  = torch.min(value_loss_casc1, value_loss_casc2)\n",
        "\n",
        "            loss = loss + value_kl_penalty + value_loss_casc\n",
        "            #print(value_kl_penalty); print(value_loss_casc)\n",
        "            total_loss.append(loss)\n",
        "            #print(f\"new loss: {loss}\\n\")\n",
        "            ### end new code ###\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kl_penalty(hidden_cascade, actual_policy, old_log_policy, beta=0.5, omega=0.5, k=3):\n",
        "    kldiv_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    loss = 0\n",
        "\n",
        "    for _ in range(len(hidden_cascade.keys())):\n",
        "        loss = - beta*(omega**i) * kldiv_loss(actual_policy, old_log_policy )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "LVPng0Sks5H-"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_casc(hidden_cascade, actual_policy, old_log_policy, omega=2.0, omega12=0.25, k=3):\n",
        "    kldiv_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "\n",
        "    loss = - omega12*kldiv_loss( actual_policy[0], old_log_policy[1] )\n",
        "\n",
        "    for i in range(len(hidden_cascade.keys())):\n",
        "        if i==0: continue\n",
        "\n",
        "        loss += - kldiv_loss( actual_policy[i], old_log_policy[i-1] )*omega\n",
        "\n",
        "        if i < ( len(hidden_cascade) -1 ): # not doing updated on the last one\n",
        "            loss += - kldiv_loss( actual_policy[i], old_log_policy[i+1] )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "iIQUzEAUuH92"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZRZqea-qeB"
      },
      "source": [
        "#Main program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6oG4o52CyUV"
      },
      "source": [
        "##Initialization for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KLi-3vI-qeC",
        "outputId": "1f95aa37-1d1c-41ec-f1d1-b7dd029e68ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_inputs1: 3, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 2, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n"
          ]
        }
      ],
      "source": [
        "num_envs = 16\n",
        "\n",
        "env1_id = \"Pendulum-v1\"\n",
        "env2_id = \"MountainCarContinuous-v0\"\n",
        "env_ids = [env1_id, env2_id ]\n",
        "\n",
        "env1 = gym.make(env1_id)\n",
        "env2 = gym.make(env2_id)\n",
        "single_envs = { env1_id: env1,\n",
        "                env2_id: env2\n",
        "               }\n",
        "\n",
        "num_inputs_1  = env1.observation_space.shape[0];    num_outputs_1 = env1.action_space.shape[0]\n",
        "num_inputs_2  = env2.observation_space.shape[0];    num_outputs_2 = env2.action_space.shape[0]\n",
        "\n",
        "print(f\"num_inputs1: {num_inputs_1}, num_outputs_1: {num_outputs_1}, env_id: {env1_id}\")\n",
        "print(f\"num_inputs2: {num_inputs_2}, num_outputs_2: {num_outputs_2}, env_id: {env2_id}\")\n",
        "\n",
        "envs1 = make_vec_env(env1_id, n_envs=num_envs)\n",
        "envs2 = make_vec_env(env2_id, n_envs=num_envs)\n",
        "\n",
        "vectorized_envs = { env1_id: envs1,\n",
        "                    env2_id: envs2\n",
        "                    }\n",
        "\n",
        "#Hyper params:\n",
        "hidden_size      = 256\n",
        "lr               = 3e-4\n",
        "num_steps        = 20\n",
        "mini_batch_size  = 5\n",
        "ppo_epochs       = 4\n",
        "#threshold_reward = 0\n",
        "env_thresholds = {env1_id:-200, env2_id:50}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFAGCHGGA4tx",
        "outputId": "e498277c-6253-4f36-9294-04d617b400f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "num_inputs1: 100, num_outputs_1: 1, env_id: Pendulum-v1\n",
            "num_inputs2: 100, num_outputs_2: 1, env_id: MountainCarContinuous-v0\n",
            "dict_keys(['network_PC', 'network_2', 'network_3', 'network_4'])\n"
          ]
        }
      ],
      "source": [
        "n_hidden_network = 4\n",
        "hidden_network = {}\n",
        "for i in range(4):\n",
        "    hidden_id = 'network'\n",
        "    if i == 0: hidden_id += '_PC'\n",
        "    else: hidden_id += '_' + str(i+1)\n",
        "\n",
        "    hidden_network[hidden_id] = ActorCritic(envs=vectorized_envs, env_ids=env_ids, hidden_size=256, lr=lr).to(device)\n",
        "\n",
        "print(hidden_network.keys())\n",
        "\n",
        "#model_PC = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)\n",
        "#model_2 = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)\n",
        "#model_3 = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)\n",
        "#model_4 = ActorCritic(envs=envs, env_ids=envs_ids, hidden_size=256, lr=lr).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPBetpLoC3sa"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPsNLqs9-qeD",
        "outputId": "14d451d5-a540-4f31-f967-1ee5ee4c0da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def swtich_enviroment( vectorized_envs, env_ids, env_index):\n",
        "    env_index += 1\n",
        "    new_id_env = env_ids[ env_index % 2 ]   # -------- the switch is only for the first two environments\n",
        "    new_env = vectorized_envs[ new_id_env ]\n",
        "    return new_env, new_id_env, env_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "id": "XUug4y3KCNMX"
      },
      "outputs": [],
      "source": [
        "def train(main_model, hidden_networks, main_env, optimizer, vect_envs, train_id_env, state, max_frames, early_stop, test_rewards, threshold_reward=-200, plot_frequency=1000, do_plot=True, all_ids=None, all_rewards=None):\n",
        "    frame_idx = 0\n",
        "    loss = 0\n",
        "    while frame_idx < max_frames and not early_stop:\n",
        "        print(f\"\\rit: {frame_idx}/{max_frames}, loss: {loss}\", end=\"\")\n",
        "        log_probs = []\n",
        "        values    = []\n",
        "        states    = []\n",
        "        actions   = []\n",
        "        rewards   = []\n",
        "        masks     = []\n",
        "        entropy = 0\n",
        "\n",
        "        for _ in range(num_steps):\n",
        "\n",
        "            model_input = main_model.create_model_input(state, train_id_env)\n",
        "            dist, value = main_model(model_input)\n",
        "\n",
        "            action = dist.sample()\n",
        "            next_state, reward, done, _ = vect_envs.step(action.cpu().numpy())\n",
        "\n",
        "            log_prob = dist.log_prob(action)\n",
        "            entropy += dist.entropy().mean()\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "            masks.append(torch.FloatTensor(1-done).unsqueeze(1).to(device))\n",
        "\n",
        "            states.append(torch.FloatTensor(model_input['state'] ).to(device) )\n",
        "            actions.append(action)\n",
        "\n",
        "            state = next_state\n",
        "            frame_idx += 1\n",
        "\n",
        "            if do_plot==True and frame_idx % plot_frequency == 0:\n",
        "                test_reward = np.mean([test_env(main_model, main_env, train_id_env, max_step=250) for _ in range(10)])\n",
        "                test_rewards.append(test_reward)\n",
        "                #plot(train_id_env, frame_idx, test_rewards)\n",
        "                plot_all(all_ids, all_rewards)\n",
        "                #print(f\"test: {test_reward}, th: {threshold_reward[train_id_env]}\")\n",
        "                if test_reward > threshold_reward[train_id_env]: early_stop = True\n",
        "\n",
        "\n",
        "        model_input = main_model.create_model_input(state, train_id_env)\n",
        "        _, next_value = main_model(model_input)\n",
        "\n",
        "        # try with simple TD-error\n",
        "        returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "        returns   = torch.cat(returns).detach()\n",
        "        log_probs = torch.cat(log_probs).detach()\n",
        "        values    = torch.cat(values).detach()\n",
        "        states    = torch.cat(states)\n",
        "        actions   = torch.cat(actions)\n",
        "        advantage = returns - values\n",
        "\n",
        "        loss = ppo_update(main_model, hidden_networks, optimizer, train_id_env, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
        "        loss = torch.mean( torch.stack(loss).float() )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-LEJNaxtdBu",
        "outputId": "38cc2d07-28d3-40ae-9888-d7d1f19f45ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded: model.pt\n",
            "saving weight model\n",
            "e: 0, actual_id_env: Pendulum-v1, env_index: 2, switch: 1\n",
            "\rit: 0/40, loss: 0"
          ]
        }
      ],
      "source": [
        "epoch = 10\n",
        "epoch_per_train = 50000\n",
        "\n",
        "swtich_counter = 0\n",
        "switch_env_frequency = 1\n",
        "\n",
        "early_stop = False\n",
        "#test_rewards = []\n",
        "\n",
        "\n",
        "env_states = {}     # contain state to pass when callicng act\n",
        "env_step   = {}     # counter for switch\n",
        "env_reward = {}     # counter for reward\n",
        "env_index  = 1\n",
        "\n",
        "\n",
        "for id_env in vectorized_envs.keys():\n",
        "    env_states[id_env] = vectorized_envs[id_env].reset()\n",
        "    env_reward[id_env] = []\n",
        "    env_step[id_env]   = 0\n",
        "\n",
        "actual_id_env = env_ids[env_index]\n",
        "actual_vectorized_env = vectorized_envs[actual_id_env]\n",
        "actual_single_env = single_envs[actual_id_env]\n",
        "\n",
        "model_PC = hidden_network['network_PC']\n",
        "model_PC.load()\n",
        "optimizer = optim.Adam(model_PC.parameters(), lr=lr)\n",
        "\n",
        "for e in range(epoch):\n",
        "    model_PC.save()\n",
        "\n",
        "    #if e > 1 and ( e %  switch_env_frequency == 0):\n",
        "    if  e %  switch_env_frequency == 0:\n",
        "        actual_vectorized_env, actual_id_env, env_index = swtich_enviroment(vectorized_envs, env_ids, env_index)\n",
        "        actual_single_env=single_envs[actual_id_env]\n",
        "        swtich_counter += 1\n",
        "\n",
        "    print(f\"e: {e}, actual_id_env: {actual_id_env}, env_index: {env_index}, switch: {swtich_counter}\")\n",
        "\n",
        "    actual_state = env_states[actual_id_env]\n",
        "    train(main_model=model_PC,\n",
        "          hidden_networks=hidden_network,\n",
        "          main_env=actual_single_env,\n",
        "          optimizer=optimizer,\n",
        "          vect_envs=actual_vectorized_env,\n",
        "          train_id_env=actual_id_env,\n",
        "          state=actual_state,\n",
        "          max_frames= 40, #epoch_per_train,\n",
        "          early_stop=early_stop,\n",
        "          test_rewards = env_reward[actual_id_env],\n",
        "          threshold_reward=env_thresholds,\n",
        "          plot_frequency=10, #10000,\n",
        "          do_plot=True,\n",
        "          all_ids=env_ids,\n",
        "          all_rewards=env_reward\n",
        "          )\n",
        "#UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
        "# return torch.tensor(encoded_states, dtype=torch.float32)\n",
        "#solve this on encoder\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}